<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[二元离散型随机变量]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E4%BA%8C%E5%85%83%E7%A6%BB%E6%95%A3%E5%9E%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[二元随机变量定义&nbsp; 二元离散随机变量定义&nbsp; 二元离散随机变量的联合概率分布律定义&nbsp; 联合概率分布律的性质&nbsp; 例&nbsp;&nbsp; 边际分布定义&nbsp;&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[连续型随机变量]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E8%BF%9E%E7%BB%AD%E5%9E%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[分布函数&nbsp; 连续随机变量&nbsp; 概率密度函数的性质&nbsp;&nbsp;&nbsp;&nbsp; 均匀分布定义&nbsp; 均匀分布性质&nbsp; 指数分布定义&nbsp; 指数分布性质&nbsp;&nbsp;&nbsp; 指数分布用途&nbsp; 正态分布定义&nbsp;&nbsp; 用途&nbsp; 标准正态分布&nbsp; 标准正态分布表&nbsp; 标准正态分布性质&nbsp; 正态分布性质&nbsp; 3σ准则&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离散型随机变量]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E7%A6%BB%E6%95%A3%E5%9E%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[离散型随机变量定义&nbsp;&nbsp;&nbsp; 0-1分布&nbsp; Bernoulli试验&nbsp;&nbsp; 二项分布&nbsp;&nbsp; 泊松分布&nbsp; 二项分布与泊松分布&nbsp;即当n&gt;10, p&lt;0.1时,二项分布B(n,p)可以用泊松分布π(np)来近似。&nbsp; 几何分布&nbsp;在重复多次的贝努里试验中，试验进行到某种结果出现第一次为止，此时的试验总次数服从几何分布。 如:射击，首次击中目标时射击的次数。]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论基础]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[频率与概率频率：频率是0~1之间的一个实数，在大量重复试验的基础上给出了随机事件发生可能性的估计。概率：当试验的次数增加时，随机事件A发生的频率的稳定值p称为概率。记为P(A)=p。 条件概率P(B|A)表示A发生的条件下, B发生的条件概率。P(B)!=P(B|A)，其原因在于事件A的发生改变了样本空间。&nbsp;&nbsp; 全概率公式与贝叶斯公式全概率公式&nbsp;&nbsp; 贝叶斯公式&nbsp; 事件独立性定义:设A，B是两随机事件，如果P(AB)=P(A)P(B)，则称A，B相互独立。两两独立并不能推出相互独立 随机变量&nbsp;随机变量分为离散型随机变量和连续性随机变量。]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手写数字分类]]></title>
    <url>%2Fexamples%2F%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言MNIST问题可以称作深度学习中的“Hello World”。它与新闻分类一样，也是一个多分类问题。本例采用MNIST数据集，并且使用卷积神经网络模型来解决手写数字识别问题。 正文数据预处理本例采用训练集样本为60000，测试集样本为10000，每张图片大小均为28*28。对训练集、测试集样本转换为4D张量，并将其标准化。对训练集、测试集标签将其向量化。 123456789(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels) 构建网络采用一层卷积层，一层池化层，一层卷积层，一层池化层，一层卷积层，两层全连接层。在最后一层卷积层中，需要对3D数据转化为1D数据。 123456789model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D(2, 2))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax')) 编译模型1234model.compile( optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 模型训练1model.fit(train_images, train_labels, epochs=5, batch_size=64) 结果与分析模型在测试集中进行评估，损失度与准确度为：0.027598469233673314 0.9916，比简单的密集连接模型的准确度97.8%要高。 源码123456789101112131415161718192021222324252627282930313233from keras import layersfrom keras import modelsfrom keras.datasets import mnistfrom keras.utils import to_categoricalimport numpy as np (train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D(2, 2))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.compile( optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(train_images, train_labels, epochs=5, batch_size=64)test_loss, test_acc = model.evaluate(test_images, test_labels)print(test_loss, test_acc) 后记理解卷积神经网络的基本概念，即特征图、卷积、最大池化。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Multiclass Classification</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perceptron]]></title>
    <url>%2Fmachine%20learning%2FPerceptron%2F</url>
    <content type="text"><![CDATA[M-P神经元神经网络中最基本的成分就是神经元（neuron）模型。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个阈值（threshold），那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。M-P神经元就是将上述情形进行抽象所得，如下图。在上述模型中，神经元收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数（activation function”）处理以产生神经元的输出。这样把许多这样的神经元按一定的层次结构连接起来，就得到了神经网络。 感知机感知机（Perceptron）是神经网络和支持向量机的基础。 感知机原理感知机是二分类的线性模型，其输入是实例的特征向量，输出的是事例的类别，分别是+1和-1，属于判别模型。假设训练数据集是线性可分的，即存在一个线性超平面能将它们分开，而感知机的学习过程就一定会收敛（converge）而求得适当的权向量w=（w1;w2;…;wn);最后求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面，如下图。如果是非线性可分的数据，感知机学习过程将会发生振荡（fluctuation），w难以稳定下来，不能求得合适解，最后无法获得超平面。 感知机结构它有两层神经元组成，输入层接收外界输入信号传递给输出层，输出层是M-P神经元，如下图 感知机学习过程对于输出，权重wi(i=1,2,…,n)以及阈值θ可通过学习得到。阈值θ可看做一个固定输入为-1.0的“哑结点”所对应的连接权重w(n+1)，这样，权重和阈值的学习就可统一为权重的学习。感知机的学习规则非常简单，对训练样例(x,y)，若当前感知机的输出为，则感知机的权重调整为：其中称为学习率（learning rate）。 感知机解决非线性问题要解决非线性可分问题，需要考虑使用多层功能神经元。如下图则可用两层感知机解决问题。输出层与输入层之间的一层神经元，被称为隐含层（hidden layer），隐含层和输出层神经元都是拥有激活函数的功能神经元 多层前馈神经网络常见的神经网络如下图所示（由两层或两层以上的感知机所组成）。每一层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构通常称为“多层前馈神经网络”（multi-layer feedforward neural networks），其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值，神经网络“学”到的东西，蕴含在连接权与阈值之中。注：“前馈”是指网络拓扑结构上不存在环或回路 参考[1]感知机原理（perceptron）[2]周志华.机器学习.北京：清华大学出版社，2016年]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BackPropagation]]></title>
    <url>%2Fmachine%20learning%2FBackPropagation%2F</url>
    <content type="text"><![CDATA[BP算法简介对于非线性可分问题，需要考虑使用多层功能神经元，因此多层网络的学习能力比单层感知机强得多。而训练多层网络，感知机的学习规则就无法适用了。目前误差逆传播（error BackPropagation）算法是迄今最成功的的神经网络学习算法。BP算法不仅可用于多层前馈神经网络，还可用于其他类型的神经网络，如递归神经网络。 BP算法推导&nbsp;&nbsp;&nbsp; BP算法工作流程1、先将输入实例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；2、计算输出层的误差，再将误差逆向传播至隐层神经元，最后根据隐层神经元的误差来对连接权和阈值进行调整；3、1,2步骤迭代循环进行，直到达到某些停止条件为止，例如训练误差已达到一个很小的值。 累积BP算法与标准BP算法BP算法的目标是要最小化训练集D上的累积误差但上述推导的是“标准BP算法”：每次仅针对一个训练样例更新连接权和阈值；“累积BP算法”是基于累积误差最小化的算法。 标准BP算法每次更新只针对单个样例，参数更新得非常频繁，，而且对不同样例进行更新得效果可能会出现“抵消”现象。因此，为了达到同样的累积误差极小点，标准BP算法往往需要进行更多次的迭代。 累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后才对参数进行更新，其参数更新的频率低得多。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会更快获得较好的解。参考[1]周志华.机器学习.北京：清华大学出版社，2016年]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[预测房价]]></title>
    <url>%2Fexamples%2F%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%2F</url>
    <content type="text"><![CDATA[前言预测房价是一个回归问题，本例采用20世纪70年代中期波士顿房价数据集（boston_housing)，主要去预测房屋价格的中位数。并已知当时该郊区的一些数据点，如犯罪率、当地房产税率等。这些数据点（特征）都有不同的取值范围。该数据集样本数量较少，只有506个，分为404个训练样本和102个测试样本。 正文数据预处理因为各个特征的取值范围差异很大，如果直接输入，网络会自动适应取值不同的数据，学习效果会很差。所以，我们需要对每个特征进行标准化。标准化：（输入的每个特征-特征平均值）/ 标准差 12345678# 数据预处理 标准化mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std 构建网络+编译本例网络采用两个隐藏层，一个输出层；隐藏层：采用Dense（全连接），隐藏单元均为64，激活函数为Relu；输出层：采用Dense，隐藏单元为1，无需激活函数，是一个线性层。这是标量回归（标量回归是预测单一连续值的回归）典型设置。 编译采用：优化器：rmsprop损失函数：MSE（mean squared error）衡量指标：MAE（mean absolute error） 1234567def build_model(): model = models.Sequential() model.add(layers.Dense(64,activation='relu',input_shape=(train_data.shape[1],))) model.add(layers.Dense(64,activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) return model 模型训练因为本例的训练集数量过少，所以采用K折交叉验证来进行模型的评估。 12345678910111213141516171819202122232425# K折交叉验证k = 4num_val_samples = len(train_data) // knum_epochs = 500all_mae_histories = []for i in range(k): print('processing fold #', i) val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[: i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[( i + 1 ) * num_val_samples:]], axis=0) model = build_model() history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1,verbose=0) mae_history = history.history['val_mean_absolute_error'] all_mae_histories.append(mae_history) 结果与分析计算所有轮次中的K折验证分数平均值 12average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)] （以前从未发现还可以这样用TAT）绘制的验证分数如下：由该图发现，前十个点的取值范围与其他点不同，需删除；纵轴的范围过大，且数据方差也过大，难以看清图的规律，故我们采用指数移动平均值（EMA）来得到光滑的曲线。 123456789def smooth_curve(points, factor=0.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points 123456smooth_mae_history = smooth_curve(average_mae_history[10:])plt.plot(range(1, len(smooth_mae_history) +1), smooth_mae_history)plt.xlabel('Epochs')plt.ylabel('Validation MAE')plt.show() 再次绘制的验证分数如下：由此发现，在第180轮后，不再显著降低（书上得出的结果和我的不同），之后开始过拟合。因此我们继续调整模型，epochs=180，并在测试集上进行测试，最后MAE为：2.7016119021995393 。在实际价格范围在10000~50000美元，预测房价与实际相差为2700美元左右。 源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from keras.datasets import boston_housingfrom keras import modelsfrom keras import layersimport numpy as npimport matplotlib.pyplot as plt(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()def build_model(): model = models.Sequential() model.add(layers.Dense(64,activation='relu',input_shape=(train_data.shape[1],))) model.add(layers.Dense(64,activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) return modeldef smooth_curve(points, factor=0.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points# 数据预处理 标准化mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std# K折交叉验证k = 4num_val_samples = len(train_data) // knum_epochs = 180all_mae_histories = []for i in range(k): print('processing fold #', i) val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[: i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[( i + 1 ) * num_val_samples:]], axis=0) model = build_model() history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1,verbose=0) mae_history = history.history['val_mean_absolute_error'] all_mae_histories.append(mae_history)"""average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]smooth_mae_history = smooth_curve(average_mae_history[10:])plt.plot(range(1, len(smooth_mae_history) +1), smooth_mae_history)plt.xlabel('Epochs')plt.ylabel('Validation MAE')plt.show()"""test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)print(test_mae_score) 后记 特征数据范围差异过大时，需要对特征进行标准化或归一化； 回归问题常用的损失函数是均方误差（MSE），常用的回归指标是平均绝对误差（MAE）； 数据过少时，可以采用K折交叉验证； 在绘制图形时，若遇到纵轴范围较大，可以采用指数移动平均值（EMA）来得到光滑的曲线。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Keras</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新闻分类]]></title>
    <url>%2Fexamples%2F%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言新闻分类是一个多分类问题，本例采用路透社数据集（reuters），该数据集将新闻分为互斥的46个不同的主题，分别有8982个训练样本和2246个测试样本。 正文数据预处理1、数据向量化。仍旧采用one-hot编码。博主发现python的赋值系统居然可以一个列表作为二维矩阵的索引值，直接全体进行赋值，以前从未在Java中感受过。2、标签向量化。因为标签值是一个标量，数据集的多个样本组成了向量。而to_categorical()则可以讲一个向量转化为由0/1组成的二维矩阵。 123456789101112131415from keras.utils.np_utils import to_categoricaldef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)# 数据向量化，转化为2D张量x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)# 标签向量化y_train = to_categorical(train_labels)y_test = to_categorical(test_labels) 构建网络本例采用两层隐藏层，一层输出层；中间两层均采用Dense（全连接），隐藏单元均为64，激活函数均为Relu；第三层输出层采用Dense，隐藏单元为46（46个新闻分类），激活函数采用softmax。 12345# 构建网络model = models.Sequential()model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))model.add(layers.Dense(64,activation='relu'))model.add(layers.Dense(46,activation='softmax')) 编译模型优化器：rmsprop损失函数：categorical_crossentropy（分类交叉熵）衡量指标：accuracy 123model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 模型训练123456789101112# 验证集x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:]# 训练模型history = model.fit( partial_x_train, partial_y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val)) 结果与分析训练与验证损失、训练与验证正确率曲线图如下所示：由此可分析，训练在第9轮后达到过拟合。为了降低过拟合，本例采用添加权重正则化、添加dropout正则化,训练轮数设为40。 123456789# 构建网络model = models.Sequential()model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu',input_shape=(10000,)))model.add(layers.Dropout(0.5))model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu'))model.add(layers.Dropout(0.5))model.add(layers.Dense(46,activation='softmax')) 观察上图，发现过拟合问题得到缓解，在第20轮后出现过拟合，因此将训练轮数设为20。 测试集损失与准确率1results = model.evaluate(x_test, y_test) 损失与准确率：[1.2500886322660099 0.7640249332146037] 预测12predictions = model.predict(x_test)print(np.argmax(predictions[0])) 源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from keras import modelsfrom keras import layersfrom keras import regularizersfrom keras.datasets import reutersfrom keras.utils.np_utils import to_categoricalimport numpy as npimport matplotlib.pyplot as pltdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)# 数据向量化，转化为2D张量x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)# 标签向量化y_train = to_categorical(train_labels)y_test = to_categorical(test_labels)# 构建网络model = models.Sequential()model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu',input_shape=(10000,)))model.add(layers.Dropout(0.5))model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu'))model.add(layers.Dropout(0.5))model.add(layers.Dense(46,activation='softmax'))# 编译模型model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])# 验证集x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:]# 训练模型history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"""# 绘制损失loss = history.history['loss']val_loss = history.history['val_loss']epochs = range(1, len(loss)+1)plt.plot(epochs, loss, 'bo', label='Training loss')plt.plot(epochs, val_loss, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show()# 绘制精确acc = history.history['acc']val_acc = history.history['val_acc']plt.clf()plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show()""""""results = model.evaluate(x_test, y_test)print(results)"""predictions = model.predict(x_test)print(np.argmax(predictions[0])) 后记降低过拟合是整个深度学习的核心问题。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Multiclass Classification</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电影评论分类]]></title>
    <url>%2Fexamples%2F%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言电影评论分类是一个二分类问题，为了将电影评论区分为正面评论和负面评论。本例采用IMDB数据集，它包含互联网电影数据库（IMDB）的50000条严重两极化评论。数据集被分为用于训练的25000条评论与用于测试的25000条评论。IMDB数据集已经经过预处理：评论（单词序列）已被转换为整数序列，其中每个整数代表字典中的某个单词（按使用频率进行排列）。 正文数据加载本例保留训练数据中前5000个最常出现的单词，低频单词将被舍弃。(因电脑原因，改为5000) 12from keras.datasets import imdb(train_data, train_lables), (test_data, test_labels) = imdb.load_data(num_words=5000) 注：实验时，发现数据下载不下来，只能通过本地下载，再将下载后的数据放在 C:\Users\Administrator.keras\datasets文件夹下 数据预处理需要将样本数据的整数序列转换为2D张量。本例采用one-hot编码，将其转换为01的向量。Dense层（全连接层）可以处理浮点数向量数据。 12345678import numpy as np def vectorize_sequences(sequences, dimension=5000): results = np.zeros((len(sequences),dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return resultsx_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data) 将标签向量化 12y_train = np.asarray(train_lables).astype('float32')y_test = np.asarray(test_lables).astype('float32') 构建网络本例采用：两个中间层（均为全连接层），隐藏单元（hidden unit）个数为16，激活函数为relu（rectified linear unit）；第三层输出一个标量，预测当前情感，激活函数为sigmoid函数； 1234567from keras import modelsfrom keras import layersmodel = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(5000,)))model.add(layers.Dense(16,activation='relu'))model.add(layers.Dense(1,activation='sigmoid')) 编译模型本例采用：损失函数：binary_crossentropy(二元交叉熵损失)优化器：rmsprop优化器 1234model.compile( optimizer='rmsprop', loss='bianry_crossentropy', metrics=['accuracy']) 训练模型123456789101112x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:]history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val,y_val)) 结果分析绘制训练损失和验证损失123456789101112131415import matplotlib.pyplot as plthistory_dict = history.historyloss_values = history_dict['loss']val_loss_values = history_dict['val_loss']epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, 'bo', label='Training loss')plt.plot(epochs, val_loss_values, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show() 绘制训练精度和验证精度12345678910plt.clf()acc = history_dict['acc']val_acc = history_dict['val_acc']plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.title('Training and validation accuracy')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show() 由两图可知，训练损失不断降低，训练精度不断升高。而验证损失却在第4轮达到最低后又继续上升，验证精度在第4轮达到最高后又继续降低。这个原因是因为过拟合，对数据过度优化，导致学到的表示仅适用于训练数据，无法泛化到训练集之外的数据。因此需要在第四轮之后停止训练。 123456history = model.fit( partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val,y_val)) 测试集损失与准确率训练轮数在四轮结束后，测试集的损失和准确率结果如下：[0.29896098415374756, 0.87688] 源代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from keras.datasets import imdbfrom keras import modelsfrom keras import layersimport numpy as npimport matplotlib.pyplot as plt# one-hot编码def vectorize_sequences(sequences, dimension=5000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results(train_data, train_labels), (test_data, test_labels) = imdb.load_data( num_words=5000)# 样本数据转换为2D张量x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)# 标签向量化y_train = np.asarray(train_labels).astype('float32')y_test = np.asarray(test_labels).astype('float32')x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:]# 模型构建model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(5000,)))model.add(layers.Dense(16,activation='relu'))model.add(layers.Dense(1,activation='sigmoid'))# 编译model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])# 训练history = model.fit(partial_x_train, partial_y_train,epochs=4,batch_size=512,validation_data=(x_val,y_val))history_dict = history.historyloss_values = history_dict['loss']val_loss_values = history_dict['val_loss']epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, 'bo', label='Training loss')plt.plot(epochs, val_loss_values, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show()plt.clf()acc = history_dict['acc']val_acc = history_dict['val_acc']plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.title('Training and validation accuracy')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show()## 预测results = model.evaluate(x_test, y_test)print(results) 后记可以通过降低过拟合来进一步优化，提高预测精度]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Keras</tag>
        <tag>Binary Classification</tag>
      </tags>
  </entry>
</search>
