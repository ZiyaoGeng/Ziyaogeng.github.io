<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[“Z=X+Y_max_min]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2FZ-X-Y-max-min%2F</url>
    <content type="text"><![CDATA[Z=X+Y定义&nbsp;&nbsp; 卷积公式&nbsp; 例&nbsp;&nbsp; 离散变量的独立和分布&nbsp; max(X,Y) min(X,Y)定义&nbsp;&nbsp; 例&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机变量的独立性]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E7%8B%AC%E7%AB%8B%E6%80%A7%2F</url>
    <content type="text"><![CDATA[随机变量的独立性定义&nbsp;&nbsp; 例&nbsp;&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二元均匀分布、二元正态分布]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E4%BA%8C%E5%85%83%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83%E3%80%81%E4%BA%8C%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[二元均匀分布定义&nbsp; 例&nbsp;&nbsp; 二元正态分布&nbsp;&nbsp;&nbsp;&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二元连续型随机变量]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E4%BA%8C%E5%85%83%E8%BF%9E%E7%BB%AD%E5%9E%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[联合概率密度函数定义&nbsp;&nbsp; 性质&nbsp;&nbsp; 例&nbsp;&nbsp; 边际概率密度定义&nbsp;&nbsp; 例&nbsp; 条件概率密度定义&nbsp;&nbsp; 例&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 二元离散型与连续型变量分布比较&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex-8 Anomaly Detection and Recommender Systems]]></title>
    <url>%2FCoursera_ML%2Fex-8%2F</url>
    <content type="text"><![CDATA[Recommender Systems主要是实现协同过滤算法对电影评分进行应用。 Movie ratings dataset Y：Y是1682*943构成的矩阵，$Y_{i,j}$代表j用户对i电影的评分（0-5）。 R：R是1682*943构成的0、1矩阵，$R_{i,j}$代表j用户是否对i电影进行评分。 Collaborative filtering learning algorithm协同过滤算法可以同时学习用户的参数和电影的特征。为了方便后续训练(x0只接受向量化参数），我们需要对两者进行合并和拆解。拆解：1234def deserialize(parm, n_users, n_movies, n_features): Theta = np.reshape(parm[:n_users * n_features], (n_users, n_features)) X = np.reshape(parm[n_users * n_features:], (n_movies, n_features)) return Theta, X 合并：1parm = np.concatenate((Theta.ravel(), X.ravel())) Collaborative filtering cost function协同过滤算法的代价函数被定义为： J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})=\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2123456def regularizer_cost(parm, Y, R, n_users, n_movies, n_features, l): Theta, X = deserialize(parm, n_users, n_movies, n_features) h = X @ Theta.T #(1682, 943) cost = np.sum(np.power(np.multiply(h - Y, R), 2)) / 2 + \ (l / 2) * np.sum(parm ** 2) return cost 注：看了作业的介绍说用循环来实现对已评价的h-y进行运算，但结果就是很慢（python能不用for循环就不用），看了大佬的代码后，发现可以用矩阵对应元素相乘的方法解决这个问题（没想到）。 Collaborative filtering gradient协同过滤算法的梯度函数为：$\frac{\partial{J}}{\partial{x_k^{(i)}}}=\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\theta_k^{j}+\lambda x_k^{(i)}$$\frac{\partial{J}}{\partial{\theta_k^{(i)}}}=\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}x_k^{(i)}+\lambda \theta_k^{(j)}$1234567891011def regularizer_gradient(parm, Y, R, n_users, n_movies, n_features, l): Theta, X = deserialize(parm, n_users, n_movies, n_features) X_grad = np.zeros(X.shape) Theta_grad = np.zeros(Theta.shape) h = X @ Theta.T #(1682, 943) X_grad = np.multiply(h - Y, R) @ Theta X_grad += l * X Theta_grad = np.multiply(h - Y, R).T @ X Theta_grad += l * Theta parm = np.concatenate((Theta_grad.ravel(), X_grad.ravel())) return parm 注：大佬在此处是最后加上愿parm*l，不过我在此处为了还原公式，分开进行计算。 Learning movie recommendationsMovie list首先，我们需要得到整个电影列表。12345678def movie_list(): movie_list = [] with open('movie_ids.txt',encoding='latin-1') as f: for line in f: line = line[line.find(' ') + 1:-1] movie_list.append(line) movie_list = np.array(movie_list) return movie_list 注：找到第一个空的索引，对电影名进行提取。 Prepare data首先当然是对所需的数据进行读入，然后对Y、R增加一列，当然这不是偏置项，这是你个人对每一部电影的评价分数，当然如果只是创建一个全0向量，就是没有对任意一部进行评价。（推荐系统的目的当然是通过训练的模型，再结合你自己的实际评分，来预测你对其他电影的评分，进而向你推荐你喜欢的电影）。最后对X、Theta随机取初始值（标准正态随机数）。此处我们选择特征为50，正则化系数为10（先如此选择）。1234567891011data = loadmat('ex8_movies.mat')Y = data['Y'] #(1682, 943) R = data['R'] #(1682, 943)ratings = np.zeros(1682)Y = np.c_[ratings, Y]R = np.c_[ratings != 0, R]X = np.random.standard_normal((n_movies, n_features)) #(1682, 10)Theta = np.random.standard_normal((n_users, n_features)) #(943, 10)n_movies, n_users = Y.shapen_features = 50l = 10 注：np.random.standard_normal Mean Normalization均值归一化的作用是，如果用户未对任意一部电影进行评分，可以让每部电影的平均分为该用户的评分。首先进行均值归一化处理，对每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值：&nbsp;如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去。12345678910def mean_norm(Y): Y = Y.astype('float') #mean = np.mean(Y,axis = 1) #mean_matrix = np.reshape(np.repeat(mean, Y.shape[1]),(Y.shape[0], Y.shape[1])) #Y -= mean_matrix #return Y, mean Y_norm = Y - Y.mean() return Y_norm, Y.mean()Y, mean = mean_norm(Y) 注：此处大佬的意思是对全部取平均分，每个评分再去减去它，与课上讲的不符啊。（另一部分代码则是还原上述的均值归一化处理）。 Train12345678910def train(parm, Y, R, n_users, n_movies, n_features, l): fmin = op.minimize(fun = regularizer_cost, x0 = parm, args=(Y, R, n_users, n_movies, n_features, l), method = 'TNC', jac = regularizer_gradient) return fminparm = np.concatenate((Theta.ravel(), X.ravel()))fmin = train(parm, Y, R, n_users, n_movies, n_features, l) Recommendations按照作业的要求，我们是依据自己对所有电影的评分进行推荐系统认为我最喜欢的前十部电影。12345678910Theta, X = deserialize(fmin.x, n_users, n_movies, n_features)pred_y = np.dot(X, Theta.T)my_pred = pred_y[:,0] + meanidx = np.argsort(my_pred)[::-1] # Descending orderprint(my_pred[idx][:10])movie_list = movie_list()for m in movie_list[idx][:10]: print(m) 当评分如下：1234567891011ratings[0] = 4ratings[6] = 3ratings[11] = 5ratings[53] = 4ratings[63] = 5ratings[65] = 3ratings[68] = 5ratings[97] = 2ratings[182] = 4ratings[225] = 5ratings[354] = 5 结果为：123456789101112[4.12536086 4.04411832 3.99323928 3.91902832 3.81689567 3.81556045 3.76602196 3.76322213 3.75903971 3.75078448]Titanic (1997)Star Wars (1977)Shawshank Redemption, The (1994)Forrest Gump (1994)Raiders of the Lost Ark (1981)Braveheart (1995)Return of the Jedi (1983)Usual Suspects, The (1995)Godfather, The (1972)Schindler&apos;s List (1993) 当未对任意一部电影进行评分，推荐结果如下：123456789101112[0.2223135 0.22231341 0.22231338 0.22231337 0.22231311 0.22231304 0.22231303 0.222313 0.222313 0.22231298]Liar Liar (1997)Titanic (1997)Scream (1996)George of the Jungle (1997)Chasing Amy (1997)Good Will Hunting (1997)Fly Away Home (1996)Con Air (1997)Starship Troopers (1997)Truth About Cats &amp; Dogs, The (1996) 推荐系统给出的评分是对每部电影的平均评分。 以上是使用大佬的均值归一化，下面使用课上讲的均值归一化进行处理（l=10），得到如下结果：123456789101112[4.46112006 4.27142573 4.21052584 4.14620017 4.01063254 3.91351821 3.89114009 3.89076407 3.79091268 3.75694315]Star Wars (1977)Titanic (1997)Raiders of the Lost Ark (1981)Return of the Jedi (1983)Empire Strikes Back, The (1980)Braveheart (1995)Godfather, The (1972)Shawshank Redemption, The (1994)Terminator 2: Judgment Day (1991)Good Will Hunting (1997) 测试结果与第一条类似。 再测试若对第一位用户进行推荐（l=10）：123456789101112[5.64227498 5.46177902 5.41247432 5.22233726 5.12619984 5.09219922 5.07283847 5.05736107 5.03524349 4.9926624 ]Star Wars (1977)Wrong Trousers, The (1993)Close Shave, A (1995)Empire Strikes Back, The (1980)Secrets &amp; Lies (1996)Return of the Jedi (1983)Lawrence of Arabia (1962)Blade Runner (1982)Fargo (1996)Leaving Las Vegas (1995) 此处想不通为什么评分大于5？？？ 再次测试对第二位用户进行推荐（l=10）：123456789101112[4.62734085 4.62437752 4.50812562 4.4862737 4.44819498 4.42677091 4.40712773 4.36196627 4.34529934 4.33993133]Titanic (1997)Fargo (1996)Schindler&apos;s List (1993)Good Will Hunting (1997)Star Wars (1977)Godfather, The (1972)Boot, Das (1981)L.A. Confidential (1997)Casablanca (1942)Close Shave, A (1995) 正确，所以第一位用户大于5的情况是什么情况？（代码查了一下午没发现任何问题）。]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex-7 K-means Clustering and Principal Component Analysis]]></title>
    <url>%2FCoursera_ML%2Fex-7%2F</url>
    <content type="text"><![CDATA[K-means ClusteringK-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此K-均值的代价函数（又称畸变函数 Distortion function）为：$J(c^{(1)},…,c^{(m)},μ_1,…,μ_K)=\dfrac{1}{m}\sum^{m}_{i=1}\left| X^{\left( i\right)}-\mu_{c^{(i)}}\right| ^{2}$其中$\mu_{c^{(i)}}$代表与$x^{(i)}$最近的聚类中心点。我们的的优化目标便是找出使得代价函数最小的 $c^{(1)}$,$c^{(2)}$,…,$c^{(m)}$和$μ^1$,$μ^2$,…,$μ^k$ 导入如下包：1234import numpy as np import matplotlib.pyplot as pltfrom PIL import Imagefrom scipy.io import loadmat Implementing K-means因此K-means算法主要分为两个步骤：(1)将样本点分配给离其最近的聚类中心（cluster centroids）点，聚成K个类。(2)重新计算每一组的平均值，聚类中心点移动到平均值的位置。 Finding closest centroids对于每一个样本$i$我们设为： $c^{(i)}:=j\:that minimizes ||x^{(i)}-\mu_j||^2,$其中$c^{(i)}$是距离$x^{(i)}$最近的聚类中心的索引，$\mu_j$是第j个聚类中心点的位置。 123456789101112def findClosestCentroids(X, centroids): idx = [] for x in X: closest = 0 small_dist = 10000 for index, cent in enumerate(centroids): dist = np.sum(np.power(x-cent, 2)) if dist &lt; small_dist: closest = index small_dist = dist idx.append(closest) return np.array(idx) Computing centroid means对于每一个聚类中心$k$我们设为： $\mu_k:=\frac{1}{|C_k|}\displaystyle\sum_{i{\in}C_k}x^{(i)}$其中$C_k$是聚类中心点k的样本数量。12345def computeCentroids(X, idx, K): centroids = [] for i in range(K): centroids.append(np.mean(X[np.where(idx == i)],axis =0)) return np.array(centroids) Kmeans将上述两个步骤相结合，形成K-means算法：123456789def runKmeans(X, centroids, max_iters, K): idx = [] #total_centroids = centroids.copy() for i in range(max_iters): idx = findClosestCentroids(X, centroids) centroids = computeCentroids(X, idx, K) #total_centroids = np.r_[total_centroids, centroids] #line_plot(total_centroids, X, idx, K) return centroids, idx Random initialization对于初始的聚类中心位置，我们需要进行随机初始化，随机为样本中某些点的位置（不重复）1234def kMeansInitCentroids(X, K): index = np.random.choice(len(X), K) initial_centroids = X[index] return initial_centroids K-means on example dataset读取数据点：1data2 = loadmat('ex7data2.mat') # X (300, 2) 散点图：12345def scatter_plot(X): plt.scatter(X[:,0], X[:,1], marker='o')scatter_plot(data2['X'])plt.show() &nbsp;K-means聚类：折线图（runKmeans函数中取消注释）：1234567891011def line_plot(total_centroids, X, idx, K): colors = ['red', 'blue', 'green'] for i in range(K): plt.scatter(X[np.where(idx == i)][:,0], X[np.where(idx == i)][:,1], c=colors[i], marker='o') plt.plot(total_centroids[::3,0::3], total_centroids[::3,1::3], c='k', marker='x') plt.plot(total_centroids[1::3,0::3], total_centroids[1::3,1::3], c='k', marker='x') plt.plot(total_centroids[2::3,0::3], total_centroids[2::3,1::3], c='k', marker='x') 注：3表示间隔。1234567K = 3max_iters = 10initial_centroids = kMeansInitCentroids(data2['X'], K)kMeansInitCentroids(data2['X'], K)centroids, idx = runKmeans(data2['X'], initial_centroids, max_iters, K)plt.show() 结果如下：&nbsp; Image compression with K-means此次对图片进行压缩，并不是通过降维，而是通过K-means方法将邻近的颜色进行聚类，从而达到压缩图片的目的。图片为128128的RGB色彩图片，保存大小为：12812838=393216 bits。本次使用K-Means方法进行压缩，图片大小为：1624+128128*4=65920bits（解释：因为RGB存储的数值为浮点型，故为8位。而压缩后的图片只需要16种颜色的RGB值，而图片中每个像素的RGB可以用聚类中心组的索引表示，即可用整型，即4位）。1234567891011121314K = 16im = Image.open('bird_small.png')A = np.array(im) # (128,128,3)A = A / 255A = A.reshape((A.shape[0]*A.shape[1],3))initial_centroids = kMeansInitCentroids(A, K)centroids, idx = runKmeans(A, initial_centroids, max_iters, K)for i in range(len(idx)): A[i] = centroids[idx[i]]A = A.reshape((128, 128, 3))A = np.uint8((A * 255))A = Image.fromarray(A)A.show() 原图与压缩后的图片对比：&nbsp; &nbsp; Principal Component Analysis主成分分析法是最常见的降维算法。它主要是将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,…,$u^{(k)}$使得总的投射误差最小。 Implementing PCANormalize the Data在使用PCA之前，我们第一步必须要将数据均值归一化（之前已经提到过）。1234def featureNormalize(X): mean = np.mean(X, axis = 0) std = np.std(X, axis = 0) return (X - mean) / std Covariance Matrix第二步我们需要计算出协方差矩阵：$\sum=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$12m = len(X)sigma = np.dot(X.T, X) / m Eigenvectors第三步我们需要计算出协方差矩阵的特征向量。我们可以利用奇异值分解（singular value decomposition）来求解，U,S,V = numpy.linalg.svd(sigma)。其中U为协方差矩阵的特征向量。12345def pca(X): m = len(X) sigma = np.dot(X.T, X) / m U, S, V = np.linalg.svd(sigma) return U, S Projecting the data onto the principal components第四步，对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$: z^{(i)}=U^{T}_{reduce}*x^{(i)}12def projectData(X, U, K): return np.dot(X, U[:, :K]) Reconstructing an approximation of the data对数据进行恢复：$x_{appox}=U_{reduce}\cdot z$,$x_{appox}\approx x$12def recoverData(Z, U, K): return np.dot(Z, U[:,:K].T) Dimensionality Reduction with PCA读取数据，并通过散点图显示：1234567def scatter_plot(X): plt.scatter(X[:,0], X[:,1], marker='o')data1 = loadmat('ex7data1.mat')X1 = data1['X'] #(50, 2)scatter_plot(X1) plt.show() &nbsp;我们按如上步骤使用PCA进行降维：123456K = 1X1_norm = featureNormalize(X1)U, S = pca(X1_norm) #(2, 2), (2, )Z = projectData(X1_norm, U, K)print(Z) 降维后的数据为：12345[[ 1.49631261] [-0.92218067] ... [ 0.36792871] [-1.44264131]] 再将其复原，与原数据的差距用散点图进行比较（当然原数据的散点图是已经经过归一化的数据）：123X_appox = recoverData(Z, U, K)plt.scatter(X_appox[:,0],X_appox[:,1], c= 'y')plt.show() &nbsp; Face Image Dataset此次实验，我们通过人脸图片来观测降维。每张图片大小为32*32的灰度图片。1234567891011121314def plot_100_image(X): size = int(np.sqrt(X.shape[1])) sample_idx = np.random.choice(np.arange(X.shape[0]), 100) sample_images = X[sample_idx, :] fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for r in range(10): for c in range(10): ax_array[r, c].imshow((sample_images[10 * r + c].reshape((size, size))).T, cmap=plt.cm.gray) # 去坐标 plt.xticks(np.array([])) plt.yticks(np.array([])) 如图所示：&nbsp; PCA on Faces123456789data_faces = loadmat('ex7faces.mat')X = data_faces['X']K =100X_norm = fn.featureNormalize(X)U, S = fn.pca(X_norm)Z = fn.projectData(X_norm, U, K)X_appox = fn.recoverData(Z, U, K)plot_100_image(X_appox)plt.show() 复原的图像：&nbsp;注：复原后的为归一化后的数据，显示的图片也是。]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex-6 Support Vector Machines]]></title>
    <url>%2FCoursera_ML%2Fex-6%2F</url>
    <content type="text"><![CDATA[Support Vector Machines导入如下包：1234import matplotlib.pyplot as plt import numpy as npfrom scipy.io import loadmatfrom sklearn import svm Example Dataset 1读取数据123data1 = loadmat('ex6data1.mat')X = np.array(data1['X'])y = np.array(data1['y']) 绘制散点图1234567def plotTrainData(X, y): positive_ex = np.array([X[i] for i in range(len(y)) if y[i] == 1]) negative_ex = np.array([X[i] for i in range(len(y)) if y[i] == 0]) plt.scatter(positive_ex[:,0],positive_ex[:,1],marker='+', c = 'black') plt.scatter(negative_ex[:,0],negative_ex[:,1],marker='o', c = 'yellow') 查看ex6data1的散点图：12plotTrainData(X, y)plt.show() 结果如下：&nbsp; 支持向量机为了使用支持向量机算法，我们导入了sklearn包中的svm模型。sklearn包内置了很多机器学习算法。1234567def SVM(X, y, C, kernel, sigma=0): if kernel == 'linear': clf = svm.SVC(C=C, kernel=kernel,decision_function_shape='ovr') else: clf = svm.SVC(C=C, kernel=kernel, gamma=sigma, decision_function_shape='ovr') clf.fit(X, y.ravel()) return clf 关于本例数数据集，通过散点图观测发现，它是一个线性可分的。并且样本数m=51，特征n=2，因此此处我们选择不带核函数的支持向量机。12C = 1clf = SVM(X, y, C, 'linear') 绘制决策边界此处借鉴了一位博主的思路（当时确实没想明白）。该方法的思路是，将整个画布的数据点全部取出来（此处每个点的间隔为0.01，近似全部取出），通过训练出来的模型去判断每个点的分类，不同的分类用不同的颜色表示，这样就能刻画出决策边界。12345678910111213141516171819def border_of_classifier(sklearn_cl, x, y): x_min, y_min = x.min(axis = 0) x_max, y_max = x.max(axis = 0) # 利用一组网格数据求出方程的值，然后把边界画出来。 x_values, y_values = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) # 计算出分类器对所有数据点的分类结果 生成网格采样 mesh_output = sklearn_cl.predict(np.c_[x_values.ravel(), y_values.ravel()]) # 数组维度变形 mesh_output = mesh_output.reshape(x_values.shape) ## 会根据 mesh_output结果自动从 cmap 中选择颜色 plt.pcolormesh(x_values, y_values, mesh_output, cmap = 'rainbow') positive_ex = np.array([x[i] for i in range(len(y)) if y[i] == 1]) negative_ex = np.array([x[i] for i in range(len(y)) if y[i] == 0]) plt.scatter(positive_ex[:,0],positive_ex[:,1],marker='+', c = 'black') plt.scatter(negative_ex[:,0],negative_ex[:,1],marker='o', c = 'yellow') 注：np.meshgrid函数是参数是两个列表，第一个输出X为按行重复参数1的所有数据直至len(参数2)行。第二个输出Y为按咧重复参数2的所有数据直至len(参数1)列。这样就能得到画布中所有的网格点。plt.pcolormesh函数是将不同的分类点以不同的颜色进行标注，cmap可指定不同的颜色分类，rainbow为自动选取颜色。结果如下所示：C = 1&nbsp;C = 100&nbsp;我们发现当C=100时，左上角的点也被划分正确，但此时已经过拟合，符合我们在课程中所学到的C越大，方差越大，越容易过拟合（因为C越大，为了最小化代价函数，需要让第一项尽可能小，所以需要点的分类尽可能正确，就会过度的拟合）。 SVM with Gaussian KernelsGaussian KernelThe Gaussian kernel function is defined as: $K_{gaussian}(x^{(i)},x^{(j)})=exp(-\frac{||x^{(i)}-x^{(j)}||^2}{2\sigma^2})=exp(-\frac{\displaystyle\sum^{n}_{k=1}(x^{(i)}_k-x^{(j)}_k)^2}{2\sigma^2})$ 12def gaussianKernel(x1, x2, sigma): return np.exp(-np.sum(np.power(x1 - x2, 2)) / (2 * np.power(sigma, 2))) 1234x1, x2 = np.array([1,2,1]),np.array([0,4,-1])sigma = 2sim = gaussianKernel(x1, x2, sigma)print(sim) 结果为：0.32465246735834974 Example Dataset 212345data2 = loadmat('ex6data2.mat') X2 = np.array(data2['X'])y2 = np.array(data2['y'])plotTrainData(X2, y2)plt.show() 散点图：&nbsp;SVM: 12345# C=100, sigma=10clf = SVM(X2, y2, C, 'rbf')print(clf.score(X2, y2))border_of_classifier(clf, X2, y2, 10)plt.show() 训练准确率：0.9698725376593279 拟合结果：&nbsp; Example Dataset 312345data3 = loadmat('ex6data3.mat')X3 = np.array(data3['X'])y3 = np.array(data3['y'])X3val = np.array(data3['Xval'])y3val = np.array(data3['yval']) 散点图：&nbsp;bestC，bestsigma：123456789101112def data3Params(X, y, Xval, yval): bestC, bestsigma, highacc = 0, 0, 0 params = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30] for C in params: for sigma in params: clf = SVM(X, y, C, 'rbf', sigma) acc = clf.score(Xval, yval) if acc &gt; highacc: highacc = acc bestC = C bestsigma = sigma return bestC, bestsigma 12bestC, bestSigma = data3Params(X3, y3, X3val, y3val)print(bestC, bestSigma) 结果为：3 30svm：1234clf = SVM(X3, y3, bestC, 'rbf', bestSigma)print(clf.score(X3val, y3val))border_of_classifier(clf, X3, y3)plt.show() 准确率：0.965拟合结果：&nbsp; Spam ClassificationPreprocessing Emails首先需要对一封邮件进行处理，处理方法如下：&nbsp;处理后，需要对每个单词进行映射为相应的数字，对照所定义的单词表vocab.txt进行映射。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def processEmail(path): email = [] vocab = [] word_indices = [] def word_stem(data): if data[-1:] == 's' and len(data) &gt; 1: data = data[:-1] if data[-2:] == 'ed': data = data[:-2] if data[-3:] == 'ing': data = data[:-3] if data[-1:] == 'e' and len(data) &gt; 1: data = data[:-1] return data with open(path) as f: for line in f: line_data = line.lower().strip().split(' ') # 去空行 if len(line_data[0]) == 0: continue else: for data in line_data: if data.isalnum() != True: # 去独立标点 if len(data) &lt; 2: continue else: # 去字符后标点 if data[len(data)-1].isalnum() != True: data = data[:-1] # 改末尾数字 if data.isdigit(): data = 'number' # 改价格 if '$' in data: data = 'dollar' # 改email地址 if '@' in data: data = 'emailaddr' # 改http if 'http' in data: data = 'httpaddr' data = word_stem(data) email.append(data) else: # 去中间数字 if data.isdigit(): data = 'number' data = word_stem(data) email.append(data) with open('vocab.txt') as f: for line in f: data = line.strip().split('\t') vocab.append(data[1]) for data in email: if data in vocab: index = vocab.index(data) + 1 word_indices.append(index) return word_indices vocab.txt：12345678910111213141 aa2 ab3 abil4 abl5 about6 abov7 absolut8 abus9 ac10 accept11 access...1898 zero1899 zip emailSample1.txt：123456789&gt; Anyone knows how much it costs to host a web portal ?&gt;Well, it depends on how many visitors you&apos;re expecting.This can be anywhere from less than 10 bucks a month to a couple of $100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..To unsubscribe yourself from this mailing list, send an email to:groupname-unsubscribe@egroups.com 处理后的结果：1[86, 916, 794, 1077, 883, 370, 1699, 790, 1822, 1831, 883, 431, 1171, 794, 592, 1676, 238, 89, 688, 1663, 1120, 1062, 1699, 375, 1162, 477, 1893, 1510, 799, 1182, 1237, 810, 1895, 1547, 1699, 1758, 1896, 688, 1676, 992, 961, 1477, 71, 530, 1699, 531] Extracting Features from Emails再将所得到的单词索引表映射为一个特征矩阵。12345def emailFeatures(word_indices): map_word_indices = np.zeros((1900,)) for i in word_indices: map_word_indices[i] = 1 return map_word_indices 结果为：1[0. 0. 0. ... 0. 0. 0.] Training SVM for Spam Classification12345678910111213Train = loadmat('spamTrain.mat')Test = loadmat('spamTest.mat')XTrain = np.array(Train['X'])yTrain = np.array(Train['y'])XTest = np.array(Test['Xtest'])yTest = np.array(Test['ytest'])clf = svm.SVC(C = 1, kernel = 'rbf', decision_function_shape='ovr')clf.fit(XTrain, yTrain)print(clf.score(XTrain, yTrain))print(clf.score(XTest, yTest)) 结果为：120.9440.953 Try your own emails对emailSample1.txt,emailSample1.txt,spamSample1.txt,spamSample2.txt进行评估预测。12345678910111213141516X = []path = ['emailSample1.txt','emailSample1.txt','spamSample1.txt','spamSample2.txt']for p in path: w_i = processEmail(p) m_w_i = emailFeatures(w_i)[1:] X.append(m_w_i)X = np.array(X)Train = loadmat('spamTrain.mat')XTrain = np.array(Train['X'])yTrain = np.array(Train['y'])clf = svm.SVC(C = 1, kernel = 'rbf', decision_function_shape='ovr')clf.fit(XTrain, yTrain)print(clf.predict(X)) 结果为：[0 0 1 0]，最后一个分类错误？]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex-5 Regularized Linear Regression and Bias v.s. Variance]]></title>
    <url>%2FCoursera_ML%2Fex-5%2F</url>
    <content type="text"><![CDATA[Regularized Linear Regression导入如下包：1234from scipy.io import loadmatimport numpy as np import matplotlib.pyplot as pltimport scipy.optimize as op Visualizing the dataset读取数据：12345678data_mat = loadmat('ex5data1.mat')Xtrain = np.array(data_mat['X'])ytrain = np.array(data_mat['y'])Xtest = np.array(data_mat['Xtest'])ytest = np.array(data_mat['ytest'])Xval = np.array(data_mat['Xval'])yval = np.array(data_mat['yval']) 画出训练集的散点图：1234567def plotScatter(Xtrain, ytrain): plt.scatter(Xtrain, ytrain, c='r', marker='x') plt.xlabel('Change in water level (x)') plt.ylabel('Water flowing out of the dam (y)')plotScatter(Xtrain, ytrain)plt.show() &nbsp; Regularized linear regression cost functionCost Function: $J(\theta)=\frac{1}{2m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\displaystyle\sum^{n}_{j=1}\theta^2_j$12345678def costFunction(theta, X, y, l=1): m = len(y) X = np.c_[np.ones((m,)), X] y = np.reshape(y,(m,)) h = np.dot(X, theta) J = np.sum(np.power(h - y,2)) / (2 * m) J += (l /(2 * m)) * np.dot(theta[1:].T, theta[1:]) return J Regularized linear regression gradientCorrespondingly, the partial derivative of regularized linear regression’s cost for $\theta_j$ is defined as: $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}\qquad for\quad j=0$ $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j\qquad for\quad j\geq1$123456789def gradient(theta, X, y, l=1): m = len(y) X = np.c_[np.ones((m,)), X] y = np.reshape(y,(m,)) h = np.dot(X, theta) grad = np.dot(X.T, h - y) / m theta[0] = 0 grad += (l / m) * theta return grad Fitting linear regressionTrain Function:123456def trainLinearRegression(initial_theta, X, y, l): fmin = op.minimize(fun = costFunction, x0=initial_theta, args=(X, y, l), jac = gradient) return fmin 1234l = 0theta = np.ones((Xtrain.shape[1] + 1,))fmin =trainLinearRegression(theta, Xtrain, ytrain, 0)print(fmin) 此处，我们另正则化参数为0，因为$\theta$的大小对低维度没有帮助。结果：1234567891011 fun: 22.373906495108923hess_inv: array([[1.03163796, 0.00618508], [0.00618508, 0.00121518]]) jac: array([-1.30694433e-09, -2.99831324e-08]) message: &apos;Optimization terminated successfully.&apos; nfev: 6 nit: 5 njev: 6 status: 0 success: True x: array([13.08790351, 0.36777923]) Fit:1234567891011def plotLine(theta, p): x_range = range(-50, 40) y_range = theta[0] for i in range(1, p + 1): y_range += np.power(x_range, i) * theta[i] plt.xticks(range(-50, 41, 10)) plt.yticks(range(-5, 41, 5)) plt.plot(x_range,y_range,c='blue')plotLine(fmin.x, 1)plt.show() 此处p为假设函数（多项式）的最高幂次，为下文服务。&nbsp; Bias-varianceLearning curves学习曲线能够很好的帮我们调试学习算法。学习曲线通过样本数量的递增，来刻画训练集的损失度和验证集的损失度，通过曲线来判断学习算法的性能。123456789101112def learningCurve(theta, Xtrain, ytrain, Xval, yval, l): error_train,error_val = np.zeros((13,)), np.zeros((13, )) for i in range(1, 13): fmin_i = trainLinearRegression(theta, Xtrain[:i,:], ytrain[:i,:], l) theta_final = fmin_i.x error_train[i] = fmin_i.fun error_val[i] = costFunction(theta_final, Xval, yval) plt.plot(range(1, 13), error_train[1:],c='blue',label='Train') plt.plot(range(1, 13), error_val[1:],c='green',label='Cross Validation') plt.xlabel('Number of training examples') plt.ylabel('Error') plt.legend() 结果为：&nbsp;我们发现，随着样本数量的增加，训练误差与验证误差都很高。因此可以判断该学习算法有高偏差（high bias）问题，即欠拟合。因此我们需要通过多项式回归来使得模型的拟合度更好。 Polynomial regression通过上述学习曲线，我们发现模型欠拟合，因此我们需要去增加特征来获得更好的模型。我们这里采用多项式回归（多项式回归本质上也是线性回归）。我们构造如下假设函数：$h_{\theta}(x)=\theta_0+\theta_1(waterLevel)+\theta_2(waterLevel)^2+…+\theta_p(waterLevel)^p$$\qquad\quad=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_px_p$ 1234def ployFeatures(X, p): for i in range(2, p + 1): X = np.c_[X, np.power(X[:,0], i)] return X 选取p=4，结果：123456789101112[[-1.59367581e+01 2.53980260e+02 -4.04762197e+03 6.45059724e+04] [-2.91529792e+01 8.49896197e+02 -2.47770062e+04 7.22323546e+05] [ 3.61895486e+01 1.30968343e+03 4.73968522e+04 1.71527069e+06] [ 3.74921873e+01 1.40566411e+03 5.27014222e+04 1.97589159e+06] [-4.80588295e+01 2.30965109e+03 -1.10999128e+05 5.33448815e+06] [-8.94145794e+00 7.99496701e+01 -7.14866612e+02 6.39194974e+03] [ 1.53077929e+01 2.34328523e+02 3.58705250e+03 5.49098568e+04] [-3.47062658e+01 1.20452489e+03 -4.18045609e+04 1.45088020e+06] [ 1.38915437e+00 1.92974986e+00 2.68072045e+00 3.72393452e+00] [-4.43837599e+01 1.96991814e+03 -8.74323736e+04 3.88057747e+06] [ 7.01350208e+00 4.91892115e+01 3.44988637e+02 2.41957852e+03] [ 2.27627489e+01 5.18142738e+02 1.17943531e+04 2.68471897e+05]] Learning Polynomial Regression通过观察上述特征值，我们发现X与y的值相差过大，因此我们需要进行特征缩放，即特征标准化（feature normalization）。对于每一个特征值，我们采用:$\frac{x^{(i)}-mean}{std}$, $std$为标准差（standard deviations），来进行特征缩放。1234567def featureNormalize(X): mean = np.mean(X, axis=0) std = np.std(X, axis=0) # (12, 8) mean = np.reshape(np.tile(mean, X.shape[0]),X.shape) std = np.reshape(np.tile(std, X.shape[0]),X.shape) return (X - mean) / std 此处axis是对X的每一列进行求mean与std。np.title(mean,x)是对mean进行复制x次。Fit:我们先通过多项式对点进行拟合。此处我们选择p=7（p=8拟合不出来啊！）。拟合时不该对数据进行特征标准化。123456Xtrain = ployFeatures(Xtrain, p)theta = np.ones((Xtrain.shape[1] + 1,))fmin = trainLinearRegression(theta, Xtrain, ytrain,l)theta = fmin.xplotLine(theta, p)plt.show() 结果如下：&nbsp;我们发现当p=7时，拟合程度过好，出现了过拟合状态。因此我们需要去调整p值，使得到的模型具有泛化的能力。最后经过调整试验，我们选择了p=4。结果如下：&nbsp;Learning Curve:123456Xtrain = ployFeatures(Xtrain, p)Xtrain = featureNormalize(Xtrain)Xval = ployFeatures(Xval, p)Xval = featureNormalize(Xval)theta = np.ones((Xtrain.shape[1] + 1,))learningCurve(theta, Xtrain, ytrain, Xval, yval, l) 结果如下：&nbsp;观察此图，我们发现训练误差一直为0，可以推出该模型高方差，即过拟合。解决过拟合问题，我们可以调整正则化参数$\lambda$（之前$\lambda=0$）来得到更好的模型。 Adjusting the reg- ularization parameter当我们改变$\lambda=1$时，拟合与学习曲线如下所示：&nbsp;&nbsp;我们发现，相比于$\lambda=0$，$\lambda=1$的效果更好，没有高方差和高偏差问题。另$\lambda=1000$，拟合图如下： &nbsp;我们发现此时拟合图已经偏离，出现欠拟合问题。 Selecting λ using a cross validation set为了找到一个合适的$\lambda$参数值，我们实现一个方法来自动选取$\lambda$值。我们先将增加假设函数幂次与特征缩放结合：12345def prepare_ploy(Xtrain, Xval, Xtest, p): Xtrain = featureNormalize(ployFeatures(Xtrain, p)) Xval = featureNormalize(ployFeatures(Xval, p)) Xtest = featureNormalize(ployFeatures(Xtest, p)) return Xtrain, Xval, Xtest 绘制随$\lambda$变化的曲线：1234567891011121314def validationCurve(theta, Xtrain, ytrain, Xval, yval): error_train, error_val = [], [] l = np.arange(0,2,0.1) for lr in l: fmin_i = trainLinearRegression(theta, Xtrain, ytrain, lr) theta_final = fmin_i.x error_train.append(fmin_i.fun) error_val.append(costFunction(theta_final, Xval, yval, lr)) plt.plot(np.arange(0,2,0.1), error_train,c='blue',label='Train') plt.plot(np.arange(0,2,0.1), error_val,c='green',label='Cross Validation') plt.xticks(l) plt.xlabel('lambda') plt.ylabel('Error') plt.legend() 结果如下图：&nbsp;我们观察发现，当$\lambda=0.3$的时候，验证损失达到最小（曲线的范围是我通过比较进行规范的，并没有选择作者给出的[0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]。注：最早之前尝试了p=3，发现曲线在0-5之间根本就没变化，后来考虑到假设函数在低维的时候，$lambda$基本不起作用，因此尝试将p=4，这时发现了曲线的变化。还有就是p的选择没有根据啊，ex-5.pdf莫名的选择了p=8，但是曲线拟合不出来，而且最后它给出的拟合图应该是p=3的吧，有点凌乱。并且作者最后的曲线给出了$\lambda=3$，而github上别人给出了$\lambda=0.3$，而我得出的也是0.3，是他们选取的p值不同么，感觉都没有说明清楚。（我发现应该是ex-5.pdf上的曲线横轴的坐标标错了吧，作者选的是0.01么？？？） Computing test set error123456Xtrain, Xval, Xtest = prepare_ploy(Xtrain, Xval, Xtest, p)theta = np.ones((Xtrain.shape[1] + 1,))#validationCurve(theta, Xtrain, ytrain, Xval, yval)#plt.show()fmin =trainLinearRegression(theta, Xtrain, ytrain, l)print(costFunction(fmin.x, Xtest, ytest, l)) 最后测试集的误差为：8.85410707764498300（3.8599的误差怎么得出来的啊！我这差的太多了。不过对比GitHub上的作者，我和他的数值是非常接近的！！！）]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex_4 Neural Networks Learning]]></title>
    <url>%2FCoursera_ML%2Fex-4%2F</url>
    <content type="text"><![CDATA[Neural NetworksDataset and The initial value数据集依旧是手写数字。定义初始值并引入包：12345678from scipy.io import loadmatimport scipy.optimize as opimport numpy as npinput_layer_size = 400hidden_layer_size = 25num_labels = 10l = 1 读取数据的方法同ex-3。1234data_mat1 = loadmat('ex4data1.mat')X = np.array(data_mat1['X'])y = np.array(data_mat1['y']) Visualizing the data与ex-3不同，此处参考了github上的代码，发现更为合适。12345678910111213def plot_100_image(X): size = int(np.sqrt(X.shape[1])) sample_idx = np.random.choice(np.arange(X.shape[0]), 100) sample_images = X[sample_idx, :] fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for r in range(10): for c in range(10): ax_array[r, c].matshow((sample_images[10 * r + c].reshape((size, size))).T, cmap=plt.cm.binary) plt.xticks(np.array([])) plt.yticks(np.array([])) &nbsp; Feedforward前向传播的作用就相当于前面Logistic回归和线性回归中的假设函数（Hypothesis），如下图所示。&nbsp; 12345678def feedForward(Theta1, Theta2, X): #(25, 401),(10, 26) m = X.shape[0] a1 = np.c_[np.ones((m,)), X] #(5000, 401) z2 = np.dot(a1, Theta1.T) #(5000, 25) a2 = np.c_[np.ones((m,)), sigmoid(z2)] #(5000, 26) z3 = np.dot(a2, Theta2.T) #(5000, 10) a3 = sigmoid(z3) #(5000, 10) return a1, z2, a2, z3, a3 返回a1, z2, a2, z3, a3是为了获取反向传播算法需要用到的参数。 Cost functionThe cost function for neural networks with regularization is given by$J(\Theta)=\frac{1}{m}\displaystyle\sum^{m}_{i=1}\displaystyle\sum^{K}_{k=1}[-y^{(i)}_klog((h_\theta(x^{(i)}))_k)-(1-y^{(i)}_k)log(1-(h_\theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\displaystyle\sum^{L-1}_{l=1}\displaystyle\sum^{s_l}_{i=1}\displaystyle\sum^{l+1}_{j=1}(\Theta^{(l)}_{i,j})^2$ 1234567891011121314151617def nncostFunction(nn_params, X, y, l): Theta1 = np.reshape(nn_params[:(input_layer_size + 1)\ * hidden_layer_size], (hidden_layer_size, input_layer_size + 1)) Theta2 = np.reshape(nn_params[(input_layer_size + 1) * \ hidden_layer_size:], (num_labels, hidden_layer_size + 1)) J = 0 m = X.shape[0] a1, z2, a2, z3, a3 = feedForward(Theta1, Theta2, X) for i in range(m): y_i = np.array([1 if label == y[i] else 0 for label in range(11)]) y_i = y_i[1:] #(10, ) J += np.dot(-y_i, np.log(a3[i,:])) - np.dot((1 - y_i), np.log(1 - a3[i,:])) J /= m theta = (np.sum(Theta1[:,1:] * Theta1[:,1:]) + np.sum(Theta2[:,1:] * \ Theta2[:,1:])) * l / (2 * m) return J + theta BackpropagationSigmoid gradientThe gradient for the sigmoid function can be computed as: $g^{‘}(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))$where $sigmoid(z)=g(z)=\frac{1}{1+e^{-z}}$12345def sigmoid(z): return 1. / (1 + np.exp(-z))def sigmoidGradient(z): return sigmoid(z) * (1 - sigmoid(z)) Random initialization随机生成初始化参数向量（权重）。$\Theta^{(l)}$的范围为$[-\epsilon_{init},\epsilon_{init}]$,此处另$\epsilon_{init}=0.12$。123456def randInitializeWeight(input_layer_size, hidden_layer_size, num_labels): epsilon_init = 0.12 W = np.random.uniform(0, 1, size = (input_layer_size+1) * \ hidden_layer_size +(hidden_layer_size + 1) * num_labels, )\ * 2 * epsilon_init - epsilon_init return W np.random.unifrom(0,1,size)函数是随机生成一个取值为[0,1]的size数组。 Backpropagation我们的最终目的是为了找到$\Theta$来$minJ(\Theta)$，因此我们需要去计算出代价函数$J(\Theta)$和偏导$\frac{\partial}{\partial\Theta^{(l)}_{i,j}}J(\Theta)$，反向传播算法就是为了计算出$\frac{\partial}{\partial\Theta^{(l)}_{i,j}}J(\Theta)$，功能同Logistic回归和线性回归的梯度函数（gradient）。我们另$\delta^{(l)}{j}$为第l层的底j个节点的误差，层数为$L$。$\delta^{(L)}=a^{(L)}-y$ $\delta^{(L-1)}=(\Theta^{(L-1)})^T\delta^{(L)}.*g^{‘}(z^{(L-1)})$ $\delta^{(L-2)}=(\Theta^{(L-2)})^T\delta^{(L-1)}.*g^{‘}(z^{(L-2)})$ …… $\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}.*g^{‘}(z^{(2)})$ 注：此处.*为矩阵各个元素对应相乘，且$g^{‘}(z^{(L-1)})=a^{(L-1)}.*(1-a^{(L-1)})$，此处z应是添加了1 伪代码如下：Training Set{$(x^{(1)},y^{(1)}),…,(x^{(m)},y^{(m)})$}Set $\Delta^{(l)}_{i,j}=0$For i=1 to m Set $a^{(1)}=x^{(i)}$ Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L Using $y^{(i)}$, compute $\delta^{(L)}-y^{(i)}$ Compute$\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$ $\Delta^{(l)}_{ij}:=\Delta^{(l)}_{ij}+a^{l}_{j}\delta^{(l+1)}_{i}$$D^{(l)}_{ij}:=\frac{1}{m}\Delta^{(l)}_{ij}+\lambda\Theta^{(l)}_{ij}$ if $j\neq0$$D^{(l)}_{ij}:=\frac{1}{m}\Delta^{(l)}_{ij}$ if $j=0$通过数学推导可得：$\frac{\partial}{\partial\Theta^{(l)}_{i,j}}J(\Theta)=D^{(l)}_{ij}$注：此处y需要进行one_hot编码。 1234567891011121314151617181920212223242526272829303132def backpropagation(nn_params, X, y, l): Theta1 = np.reshape(nn_params[:(input_layer_size + 1)\ * hidden_layer_size], (hidden_layer_size, input_layer_size + 1)) Theta2 = np.reshape(nn_params[(input_layer_size + 1) * \ hidden_layer_size:], (num_labels, hidden_layer_size + 1)) m = X.shape[0] a1, z2, a2, z3, a3 = feedForward(Theta1, Theta2, X) # one_hot one_hot_y = np.array([[1 if y[i] == j else 0 for j in range(11)]for i in range(len(y))]) one_hot_y = one_hot_y[:,1:] delta1 = np.zeros(Theta1.shape) # (25, 401) delta2 = np.zeros(Theta2.shape) # (10, 26) for i in range(m): a1_i = np.reshape(a1[i,:],(1, len(a1[0]))) #(1, 401) z2_i = np.reshape(z2[i,:],(1, len(z2[0]))) #(1, 25) a2_i = np.reshape(a2[i,:],(1, len(a2[0]))) #(1, 26) a3_i = np.reshape(a3[i,:],(1, len(a3[0]))) #(1, 10) y_i = one_hot_y[i, :] d3_i = a3_i - y_i #(1, 10) z2_i = np.c_[np.ones((1,)), z2_i] d2_i = np.dot(Theta2.T, d3_i.T).T * sigmoidGradient(z2_i) #(1, 26) delta1 = delta1 + np.dot(d2_i[:,1:].T,a1_i) delta2 = delta2 + np.dot(d3_i.T, a2_i) delta1 /= m delta2 /= m delta1[:,1:] = delta1[:,1:] + (Theta1[:,1:] * l) / m delta2[:,1:] = delta2[:,1:] + (Theta2[:,1:] * l) / m grad = np.concatenate((delta1.flatten(), delta2.flatten())) return grad Learning parameters using minimize12345678def nn_train(X,y): fmin = op.minimize(fun=nncostFunction, x0=nn_params, args=(X, y, l), method='TNC', jac=backpropagation, options=&#123;'maxiter': 250&#125;) return fminnn_params = randInitializeWeight(input_layer_size, hidden_layer_size, num_labels)print(nn_train(X, y)) 设置最大的迭代次数为250次。结果如下：12345678910 fun: 0.33431033764051943 jac: array([ 1.80122733e-04, 2.91512941e-07, -2.41860480e-07, ..., 7.21952980e-05, 4.92183397e-06, -3.14373459e-05])message: &apos;Max. number of function evaluations reached&apos; nfev: 250 nit: 20 status: 3success: False x: array([-1.10232549e+00, 1.45756471e-03, -1.20930240e-03, ..., -5.52497298e+00, -1.97210589e+00, -1.52716817e+00]) 代价函数为：0.33，这是一个不错的指标。 Predict12345678910111213def get_accuracy(y_pred, y): correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)] accuracy = (sum(correct) / float(len(correct))) print ('accuracy = &#123;0&#125;%'.format(accuracy * 100))theta1 = np.reshape(fmin.x[:(input_layer_size + 1)\ * hidden_layer_size], (hidden_layer_size, input_layer_size + 1))theta2 = np.reshape(fmin.x[(input_layer_size + 1) * \ hidden_layer_size:], (num_labels, hidden_layer_size + 1))a1, z2, a2, z3, a3 = feedForward(theta1, theta2, X)y_pred = np.argmax(a3, axis=1) + 1get_accuracy(y_pred, y) 最后预测的准确率为：accuracy = 99.3%。使用sklearn包来进行评价。12from sklearn.metrics import classification_reportprint(classification_report(y, y_pred)) 结果如下：12345678910111213141516 precision recall f1-score support 1 0.99 0.99 0.99 500 2 0.99 0.99 0.99 500 3 0.99 0.98 0.99 500 4 0.99 0.99 0.99 500 5 0.99 1.00 1.00 500 6 1.00 0.99 1.00 500 7 0.99 0.99 0.99 500 8 0.99 1.00 0.99 500 9 0.99 0.98 0.98 500 10 0.99 1.00 0.99 500 accuracy 0.99 5000 macro avg 0.99 0.99 0.99 5000weighted avg 0.99 0.99 0.99 5000 故已经成功实现基于反向传播的神经网络算法，用于实现手写数字识别。 Visualizing the hidden layer12345678910111213141516def plot_hidden_layer(theta): Theta1 = np.reshape(nn_params[:(input_layer_size + 1)\ * hidden_layer_size], (hidden_layer_size, input_layer_size + 1)) hidden_layer = Theta1[:, 1:] fig, ax_array = plt.subplots(nrows=5, ncols=5, sharey=True, sharex=True, figsize=(5, 5)) for r in range(5): for c in range(5): ax_array[r, c].matshow(hidden_layer[5 * r + c].reshape((20, 20)), cmap=plt.cm.binary) plt.xticks(np.array([])) plt.yticks(np.array([]))plot_hidden_layer(fmin.x)plt.show() &nbsp;]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex_3 Multi-class Classification and Neural Networks]]></title>
    <url>%2FCoursera_ML%2Fex-3%2F</url>
    <content type="text"><![CDATA[Multi-class Classification以下代码引用如下包：123456import scipyimport scipy.optimize as opfrom scipy.io import loadmatfrom PIL import Imageimport pandas as pd import numpy as np Dataset该数据集是由5000个样本组成的手写数字组成的，并且保存为.mat的Matlab矩阵格式。首先我们需要对数据进行提取，采用scipy.io库中的loadmat函数。12from scipy.io import loadmatdata_mat = loadmat('./ex3data1.mat') 查看其中的关键词：1print(data_mat.keys()) dict_keys([&#39;__header__&#39;, &#39;__version__&#39;, &#39;__globals__&#39;, &#39;X&#39;, &#39;y&#39;])我们选择提取X，y，并将其转化为数组并将y向量化。123X = np.array(pd.DataFrame(data_mat['X']))y = np.array(pd.DataFrame(data_mat['y']))y = y.flatten() 查看X，y的结构1print(X.shape, y.shape) 结果：(5000, 400) (5000,) X：5000个样本，特征有400个，因为每张图片的大小为20*20的灰度图，共有400个像素值。 y：5000个样本，值由1-10组成，其中10代表0（因为Matlab中索引从1开始） Visualizing the data此处的灰度图片，就是由0-255的二维矩阵组成。我们首先随机提取100张图片，将其内容放在一个(2010+pad\11, 20*10+pad*11)的矩阵中（pad为边界），然后通过PIL的Image的fromarray函数将其转化为可输出的图片。1234567891011121314151617181920def displayData(X): pad, row, col = 2, -1, 0 X_shuffle = np.copy(X) np.random.shuffle(X_shuffle) matrix = np.ones((20*10+pad*11,20*10+pad*11)) for i in range (0,100): if i % 10 == 0: row += 1 col = 0 matrix[row*20+(row+1)*pad:(row+1)*20+(row+1)*pad, col*20+(col+1)*pad:(col+1)*20+(col+1)*pad] \ = X_shuffle[i].reshape((20, 20)).T col += 1 else: matrix[row*20+(row+1)*pad:(row+1)*20+(row+1)*pad, col*20+(col+1)*pad:(col+1)*20+(col+1)*pad] \ = X_shuffle[i].reshape((20, 20)).T col += 1 matrix *= 255 picture = Image.fromarray(matrix) picture.show() picture.close() 如下图所示：&nbsp; Vectorizing Logistic RegressionPrepare首先与之前相同，需对特征矩阵进行扩充一列全为1的向量，因为线性模型中含有偏置顶。1X = np.c_[np.ones((len(y), 1)), X] Sigmoid同Logistic Regression。 $g(z)=\frac{1}{1+e^{-z}}$12def sigmoid(z): return 1./(1 + np.exp(-z)) Vectorizing the cost function此处与之前所做的Logistic Regression中的代价函数相同，不在赘述。(不过返回值少了个括号，导致我的准确率一直不对，检查了小半天才看出来，TAT) $J(\theta)=\frac{1}{m}\displaystyle\sum^{m}_{i=1}{[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)}log(1-h_\theta(^{(i)}))]}+\frac{\lambda}{2m}\displaystyle\sum^{n}_{j=1}\theta^2_j$12345def cost(theta, X, y, l = 1): h = sigmoid(np.dot(X, theta)) return (-np.dot(y, np.log(h)) - \ np.dot((1-y), np.log(1-h)))/ len(y) + \ l * theta[1:].T.dot(theta[1:]) / (2 * len(y)) Vectorizing the gradient梯度函数也与之前相同。 $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}\qquad for\quad j=0$ $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}++\frac{\lambda}{m}\theta_j\qquad for\quad j\geq1$123456def gradient(theta, X, y, l = 1): h = sigmoid(np.dot(X, theta)) theta1 = theta * l / len(y) theta1[0] = 0 grad = np.dot(X.T, h - y) / len(y) + theta1 return grad.flatten() One-vs-all Classification在多分类学习中，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。多分类的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解。先对问题进行拆分，然后为拆除的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的分类结果。最经典的拆分策略有三种：”一对一“（One vs. One），”一对其余“（One vs. Rest，亦称One vs. All），“多对多”（Many vs. Many）。OvR（OvA）是每次将一个类的样例作为正例、所有其他类的样例作为反例来训练N个分类器。（以上摘自周志华.《机器学习》）此处我们选择OvA，所以我们需要训练10个分类器，每次把一类作为正例，即正确的数字（标为1），其余作为反例（标为0），将其进行保存。123456789def oneVsAll(X, y, num_labels): all_theta = np.zeros((num_labels, X.shape[1])) for K in range(1, num_labels + 1): initial_theta = np.zeros(X.shape[1],); y_K = np.array([1 if label == K else 0 for label in y]) fmin = op.minimize(fun=cost, x0=initial_theta, args=(X, y_K, 1), method='TNC', jac=gradient) all_theta[K - 1,:] = fmin.x return all_theta 注：关于for循环，我们采用1-10，因为此处以10代表0。 One-vs-all Prediction将所得到的参数矩阵$\theta$与X做矩阵乘法，再用Sigmoid将每个分类的值转化为一个接近0或1的值，最后将10个分类器进行集成，返回每个样本中，值最大的索引，即所预测的数字。（此处+1是因为从零开始，而y是从1开始）1234def preictOneVsAll(all_theta, X): h = sigmoid(X.dot(all_theta.T)) h_argmax = np.argmax(h, axis=1) return h_argmax + 1 最后计算其分类模型的准确率。12345y_pred = preictOneVsAll(all_theta, X)correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]accuracy = (sum(correct) / float(len(correct)))print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) 注：zip(y_pred, y)是将其转化为一个元组列表，方便计算。准确率为：accuracy = 94.46%，与pdf上结果相近。 Neural NetWorkModel representation神经网络模型：&nbsp; Feedforward Propagation and Prediction12345678910111213141516171819202122232425262728from scipy.io import loadmatimport numpy as npdef sigmoid(z): return 1. / (1 + np.exp(-z))def predict(Theta1, Theta2, X, y): X = np.c_[np.ones(X.shape[0], ), X] hidden = sigmoid(np.dot(X, Theta1.T)) hidden = np.c_[np.ones(hidden.shape[0],), hidden] output = sigmoid(np.dot(hidden, Theta2.T)) y_pred = np.argmax(output, axis=1) + 1 return y_preddata_mat1 = loadmat('./ex3data1.mat')data_mat2 = loadmat('./ex3weights.mat')X = np.matrix(data_mat1['X'])y = np.matrix(data_mat1['y'])Theta1 = np.matrix(data_mat2['Theta1'])Theta2 = np.matrix(data_mat2['Theta2'])y_pred = predict(Theta1, Theta2, X, y)correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]accuracy = sum(correct) / len(correct)print('accuracy=&#123;0&#125;%'.format(accuracy * 100)) 最后预测结果准确率为：97.52%，可见神经网络模型所预测的准确度的确比Logistic回归预测的准确率高，但它容易受过拟合的影响。]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex_2 Logistic Regression]]></title>
    <url>%2FCoursera_ML%2Fex-2%2F</url>
    <content type="text"><![CDATA[Logistic RegressionVisualizing the dataRead the data采用pandas.read_csv()方法来读取ex2data1.txt文件。123import pandas as pddata = pd.read_csv('ex2data1.txt', names=['exam1', 'exam2', 'admitted']) 12# 查看data的前几项print(data.head()) 123456 exam1 exam2 admitted0 34.623660 78.024693 01 30.286711 43.894998 02 35.847409 72.902198 03 60.182599 86.308552 14 79.032736 75.344376 1 12# 查看data的结构print(data.describe()) 123456789 exam1 exam2 admittedcount 100.000000 100.000000 100.000000mean 65.644274 66.221998 0.600000std 19.458222 18.582783 0.492366min 30.058822 30.603263 0.00000025% 50.919511 48.179205 0.00000050% 67.032988 67.682381 1.00000075% 80.212529 79.360605 1.000000max 99.827858 98.869436 1.000000 Plotting the scatter plot绘制散点图，需将0和1两种情况分开绘制。故需将原数据依照admitted的数据（0/1）将数据进行拆分为positice和negative。 12345678910111213# 绘制散点图import matplotlib.pyplot as pltpositive = data[data.admitted.isin(['1'])]negative = data[data.admitted.isin(['0'])]plt.scatter(positive['exam1'], positive['exam2'], marker='+', c='black', label='Admitted')plt.scatter(negative['exam1'], negative['exam2'], marker='o', c='y', label='Not admitted')plt.legend()plt.show() 散点图如下所示：&nbsp; ImplementationHypothesis and Sigmoidlogistic regression hypothesis is defined as: $h_\theta(x)=g(\theta^Tx)$ The sigmoid function is defined as: $g(z)=\frac{1}{1+e^{-z}}$1234import numpy as np def sigmoid(z): return 1./(1 + np.exp(-z)) Cost function and gradientthe cost function in logistic regression is: $J(\theta)=\frac{1}{m}\displaystyle\sum^{m}_{i=1}{[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)}log(1-h_\theta(^{(i)}))]}$ the gradient of the cost is a vector of the same length is defined as follows: $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}$123456789def costFunction(theta, X, y): h = sigmoid(np.dot(X, theta)) J = (-np.dot(y,np.log(h)) - np.dot((1 - y),np.log(1-h))) / len(y) return Jdef gradient(theta, X, y): h = sigmoid(np.dot(X, theta)) grad = np.dot(X.T, h - y) / len(y) return grad.flatten() Learning parameters using minimize使用scipy.optimize中的minimize来代替Matlab/Octave中的minunc，去学习找到让costFunction最小的$\theta$。查看op.minimize()方法的信息。12import scipy.optimize as opprint(np.info(op.minimize) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667 minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)Minimization of scalar function of one or more variables.Parameters----------fun : callable The objective function to be minimized. ``fun(x, *args) -&gt; float`` where x is an 1-D array with shape (n,) and `args` is a tuple of the fixed parameters needed to completely specify the function.x0 : ndarray, shape (n,) Initial guess. Array of real elements of size (n,), where &apos;n&apos; is the number of independent variables.args : tuple, optional Extra arguments passed to the objective function and its derivatives (`fun`, `jac` and `hess` functions).method : str or callable, optional Type of solver. Should be one of - &apos;Nelder-Mead&apos; :ref:`(see here) &lt;optimize.minimize-neldermead&gt;` - &apos;Powell&apos; :ref:`(see here) &lt;optimize.minimize-powell&gt;` - &apos;CG&apos; :ref:`(see here) &lt;optimize.minimize-cg&gt;` - &apos;BFGS&apos; :ref:`(see here) &lt;optimize.minimize-bfgs&gt;` - &apos;Newton-CG&apos; :ref:`(see here) &lt;optimize.minimize-newtoncg&gt;` - &apos;L-BFGS-B&apos; :ref:`(see here) &lt;optimize.minimize-lbfgsb&gt;` - &apos;TNC&apos; :ref:`(see here) &lt;optimize.minimize-tnc&gt;` - &apos;COBYLA&apos; :ref:`(see here) &lt;optimize.minimize-cobyla&gt;` - &apos;SLSQP&apos; :ref:`(see here) &lt;optimize.minimize-slsqp&gt;` - &apos;trust-constr&apos;:ref:`(see here) &lt;optimize.minimize-trustconstr&gt;` - &apos;dogleg&apos; :ref:`(see here) &lt;optimize.minimize-dogleg&gt;` - &apos;trust-ncg&apos; :ref:`(see here) &lt;optimize.minimize-trustncg&gt;` - &apos;trust-exact&apos; :ref:`(see here) &lt;optimize.minimize-trustexact&gt;` - &apos;trust-krylov&apos; :ref:`(see here) &lt;optimize.minimize-trustkrylov&gt;` - custom - a callable object (added in version 0.14.0), see below for description. If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``, depending if the problem has constraints or bounds.jac : &#123;callable, &apos;2-point&apos;, &apos;3-point&apos;, &apos;cs&apos;, bool&#125;, optional Method for computing the gradient vector. Only for CG, BFGS, Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov, trust-exact and trust-constr. If it is a callable, it should be a function that returns the gradient vector: ``jac(x, *args) -&gt; array_like, shape (n,)`` where x is an array with shape (n,) and `args` is a tuple with the fixed parameters. Alternatively, the keywords &#123;&apos;2-point&apos;, &apos;3-point&apos;, &apos;cs&apos;&#125; select a finite difference scheme for numerical estimation of the gradient. Options &apos;3-point&apos; and &apos;cs&apos; are available only to &apos;trust-constr&apos;. If `jac` is a Boolean and is True, `fun` is assumed to return the gradient along with the objective function. If False, the gradient will be estimated using &apos;2-point&apos; finite difference estimation.Returns-------res : OptimizeResult The optimization result represented as a ``OptimizeResult`` object. Important attributes are: ``x`` the solution array, ``success`` a Boolean flag indicating if the optimizer exited successfully and ``message`` which describes the cause of the termination. See `OptimizeResult` for a description of other attributes. 通过以上可知： fun：为costFunction，costFunction的参数值theta必须在第一位； x0：为$theta$，即所需要学习的参数值； args：为(X, y)，是一个元组； method：为可选择的方法； jac：为梯度函数 123Result = op.minimize(fun = Fun.costFunction, x0 = theta, args = (X, y), method = 'TNC', jac = Fun.gradient) 最后，所得结果为:12345678 fun: 0.20349770158947492 jac: array([9.32681149e-09, 1.15776343e-07, 4.86078485e-07])message: &apos;Local minimum reached (|pg| ~= 0)&apos; nfev: 36 nit: 17 status: 0success: True x: array([-25.16131854, 0.20623159, 0.20147149]) costFunction的值为0.203，向量$\theta$为x。 Evaluating logistic regressionFor a student with an Exam 1 score of 45 and an Exam 2 score of 851234def predict(X,theta): return sigmoid(theta[0] + theta[1] * X[0] + theta[2] * X[1])print(Fun.predict([45,85],theta)) 所得结果为：0.7762906217710582 Plot the DecisionBoundary123456def plotDecisionBoundary(theta): x = range(30, 100, 10) Y = -(theta[0] + theta[1]*x)/theta[2] plt.plot(x, Y) plt.xlabel('Exam 1 score') plt.ylabel('Exam 2 score') 拟合结果为：&nbsp; Regularized Logistic RegressionVisualizing the dataRead the data采用pandas.read_csv()方法来读取ex2data2.txt文件。123import pandas as pddata = pd.read_csv('ex2data2.txt', names=['test1', 'test2', 'accepted']) Plotting the scatter plot同上12345678910111213141516# 绘制散点图import matplotlib.pyplot as plt positive = data[data.accepted.isin(['1'])]negative = data[data.accepted.isin(['0'])]plt.scatter(positive['test1'], positive['test2'], marker='+', c='black', label='y=1')plt.scatter(negative['test1'], negative['test2'], marker='o', c='y', label='y=0')plt.xlabel('Microchip Test 1')plt.ylabel('Microchip Test 2')plt.legend()plt.show() 散点图如下图所示：&nbsp; Feature mapping拟合这个散点图最好创造一个更高维度假设函数（Hypoththesis），如下图：&nbsp;12345def mapFeature(X): for i in range(2,7): for j in range(0, i + 1): X = np.c_[X, np.power(X[:, 1],i - j)*np.power(X[:, 2], j)] return X 对ex2data2中数据映射为28维度，结果如下：12345678910111213[[ 1.00000000e+00 5.12670000e-02 6.99560000e-01 ... 6.29470940e-04 8.58939846e-03 1.17205992e-01] [ 1.00000000e+00 -9.27420000e-02 6.84940000e-01 ... 1.89305413e-03 -1.39810280e-02 1.03255971e-01] [ 1.00000000e+00 -2.13710000e-01 6.92250000e-01 ... 1.04882142e-02 -3.39734512e-02 1.10046893e-01] ... [ 1.00000000e+00 -4.84450000e-01 9.99270000e-01 ... 2.34007252e-01 -4.82684337e-01 9.95627986e-01] [ 1.00000000e+00 -6.33640000e-03 9.99270000e-01 ... 4.00328554e-05 -6.31330588e-03 9.95627986e-01] [ 1.00000000e+00 6.32650000e-01 -3.06120000e-02 ... 3.51474517e-07 -1.70067777e-08 8.22905998e-10]] Cost function and gradientthe regularized cost function in logistic regression is： $J(\theta)=\frac{1}{m}\displaystyle\sum^{m}_{i=1}{[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)}log(1-h_\theta(^{(i)}))]}+\frac{\lambda}{2m}\displaystyle\sum^{n}_{j=1}\theta^2_j$ 我们不需要对$\theta_0$进行正则化。The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows: $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}\qquad for\quad j=0$ $\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{1}{m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}++\frac{\lambda}{m}\theta_j\qquad for\quad j\geq1$ 12345678910111213def costFunctionReg(theta, X, y, l = 1): h = sigmoid(np.dot(X, theta)) J = (-np.dot(y,np.log(h)) - np.dot((1 - y),np.log(1-h))) / len(y) + \ theta[1:].T.dot(theta[1:]) * (l / (2 * len(y))) return Jdef gradientReg(theta, X, y, l = 1): h = sigmoid(np.dot(X, theta)) # 不惩罚第一项 theta1 = theta * l / len(y) theta1[0] = 0 grad = np.dot(X.T, h - y) / len(y) + theta1 return grad.flatten() Learning parameters using minimize同上1234# 此处lambda取1Result = op.minimize(fun = Fun.costFunctionReg, x0 = theta, args = (X, y, 1), method = 'TNC', jac = Fun.gradientReg) 12345678910111213141516171819 fun: 0.5290027299645224 jac: array([-2.15176034e-06, 6.79432526e-07, -3.49220230e-07, 8.75696900e-07, -4.08192926e-08, -9.33913702e-07, -5.14416679e-07, 1.69767803e-08, 1.53043963e-08, -9.72817646e-07, 6.96367745e-08, 3.55014390e-08, -2.79877300e-07, 1.79619883e-07, 2.33072525e-07, 1.47201334e-07, -2.12252965e-07, 6.16789973e-07, -9.25472935e-08, -5.27805873e-08, -1.48180719e-06, 2.31352665e-07, 1.80331319e-07, -1.31371493e-07, -7.18038233e-08, -4.12162550e-07, 1.65946370e-08, -7.35043065e-07])message: &apos;Converged (|f_n-f_(n-1)| ~= 0)&apos; nfev: 32 nit: 7 status: 1success: True x: array([ 1.27271026, 0.62529965, 1.18111686, -2.019874 , -0.91743189, -1.43166928, 0.12393228, -0.36553118, -0.35725405, -0.17516292, -1.45817008, -0.05098418, -0.61558557, -0.27469166, -1.19271298, -0.24217841, -0.20603303, -0.04466177, -0.27778947, -0.29539513, -0.45645982, -1.04319153, 0.02779373, -0.29244865, 0.01555759, -0.32742403, -0.14389149, -0.92467488]) Plotting the decision boundary我们将分类器的预测结果在一个均匀间隔的网格上绘制出非线性决策边界，然后绘制出预测值从y = 0变化到y = 1的等值线图 12345678910def plotDecisionBoundaryReg(theta): x = np.linspace(-1, 1.5, 250) x1, x2 = np.meshgrid(x, x) # x1.ravel()是将其向量化 x3 = np.array([x1.ravel(),x2.ravel()]).T x3 = np.c_[np.ones((len(x3), 1)), x3] z = mapFeature(x3) z = z.dot(theta) z = z.reshape(x1.shape) plt.contour(x1,x2,z,0) $\lambda=1$的拟合结果为：&nbsp;$\lambda=0$的拟合结果为：&nbsp;我们发现$\lambda=0$已经过拟合$\lambda=100$的拟合结果为：&nbsp;我们发现$\lambda=100$已经欠拟合]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex_1]]></title>
    <url>%2FCoursera_ML%2Fex-1%2F</url>
    <content type="text"><![CDATA[Linear regression with one variablePlotting the DataRead the Data使用pandas库中的read_csv将数据读入。1234import pandas as pddata = pd.read_csv('./ex1data1.txt', names=['Population','Profit']) Plotting the Scatter Plot12345678import matplotlib.pyplot as pltplt.scatter(data['Population'], data['Profit'], marker='x', c='r')plt.xlabel('Population of City in 10,000s')plt.ylabel('Profit in $10,000s')plt.legend()plt.show() 散点图如下所示：&nbsp; Gradient DescentUpdate EquationsThe objective of linear regression is to minimize the cost function $J(\theta)=\frac{1}{2m}\displaystyle\sum^{m}_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$ where the hypothesis $h_\theta(x)$ is given by the linear model $h_\theta(x)=\theta^Tx=\theta_0+\theta_1x_1$ Recall that the parameters of your model are the $\theta_j$ values. These are the values you will adjust to minimize cost $J(\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update $\theta_j:=\theta_j-\alpha\frac{1}{m}\displaystyle\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$ With each step of gradient descent, your parameters $\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\theta)$. Implementation初始参数为0向量，学习率(learning rate)$\alpha$为0.01，迭代次数为150012345import numpy as nptheta = np.zeros((2,))iterations = 1500alpha = 0.01 Computing the cost $J(\theta)$关于代价函数（cost function），在回归任务中均方误差是是最常用的性能度量，我们试图让均方误差最小化。而基于均方误差最小化来进行模型求解的方法为“最小二乘法”（least square method）。在线性回归中，最小二乘法就是试图找到一条直线，是所有样本到直线上的欧式距离之和最小。1234def computeCost(theta, X, y): h = np.dot(X, theta) # np.sum为向量相加 return np.sum(np.power(h-y,2)) / (2 * len(y)) Gradient descent假设$\alpha=0.01,iterations=1500,$123456789101112131415def gradientDescent(X, y, theta, alpha, num_iters): for iters in range(num_iters): h = np.dot(X, theta) theta -= alpha * np.dot(X.T, (h - y)) / len(y)theta = np.zeros((2,))iterations = 1500alpha = 0.01y = np.array(data.iloc[:,1])X = np.array(data.iloc[:,0])X = np.c_[np.ones((len(y), 1)), X]# gradientDescentgradientDescent(X, y, theta, alpha, iterations) Plotting the linear fit1234def plotLinearPlot(theta): x = range(4,25,2) Y = theta[0] + theta[1]*x plt.plot(x, Y, label='Linear regression') 拟合结果如下：&nbsp; Visualizing $J(\theta)$&nbsp; Linear regression with multiple variablesFeature Normalization因为两个特征的数值相差近1000倍，故我们需要对其进行特征缩放（feature scaling），来加快梯度下降的收敛速度。对于每一个特征值，我们采用:$\frac{x^{(i)}-mean}{std}$, $std$为标准差（standard deviations），来特征缩放。123456def featureNormalize(X): mean = np.ones((len(X), 2)) * np.array([np.mean(X[:, 0]), np.mean(X[:, 1])]) std = np.ones((len(X), 2)) * np.array([np.std(X[:, 0]), np.std(X[:, 1])]) return (X - mean) / std Gradient Descent1234567891011121314151617181920212223import numpy as npcost = []def computeCostMulti(theta, X, y): h = np.dot(X, theta) return np.sum(np.power(h - y, 2)) / (2 * len(y))def gradientDescentMulti(theta, X, y, alpha, num_iters): for iter in range(num_iters): cost.append(computeCostMulti(theta, X, y)) h = np.dot(X, theta) theta -= alpha * np.dot(X.T, h - y) / len(y) return thetaX = np.array([data['size'], data['num']]).TX = X.astype('float')X = featureNormalize(X)X = np.c_[np.ones((len(X), 1)), X]y = np.array(data.iloc[:,2])theta = np.zeros((len(X[0]), ))theta = gradientDescentMulti(theta, X, y, 0.03, 50) Selecting learning rates当$\alpha$=0.3时，cost随迭代次数的变化曲线为：&nbsp;当$\alpha$=0.1时，cost随迭代次数的变化曲线为：&nbsp;当$\alpha$=0.03时，cost随迭代次数的变化曲线为：&nbsp;当$\alpha$=0.01时，cost随迭代次数的变化曲线为：&nbsp;由以上可以得出，$\alpha=0.1$是一个较为合适的学习率。]]></content>
      <categories>
        <category>Coursera_ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera_ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine_Learning_week1]]></title>
    <url>%2Fmachine%20learning%2Fweek1%2F</url>
    <content type="text"><![CDATA[机器学习定义【by Tom Mitchell】A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。我认为经验E就是程序上万次的自我练习的经验而任务T就是下棋。性能度量值P呢，就是它在与一 些新的对手比赛时，赢得比赛的概率。 监督学习与无监督学习监督学习（supervised learning）监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，给出一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。监督学习的代表是分类（classification）与回归（regression） 无监督学习（unsupervised learning）输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类试图使类内差距最小化，类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。非监督学习目标不是告诉计算机怎么做，而是让它（计算机）自己去学习怎样做事情。无监督学习的代表是聚类（clustering） 线性回归（Linear Regression）]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二元离散型随机变量]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E4%BA%8C%E5%85%83%E7%A6%BB%E6%95%A3%E5%9E%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[二元随机变量定义&nbsp; 二元离散随机变量定义&nbsp; 二元离散随机变量的联合概率分布律定义&nbsp; 联合概率分布律的性质&nbsp; 例&nbsp;&nbsp; 边际分布定义&nbsp;&nbsp; 条件分布定义&nbsp;&nbsp; 例&nbsp;&nbsp;&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[连续型随机变量]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E8%BF%9E%E7%BB%AD%E5%9E%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[分布函数&nbsp; 连续随机变量&nbsp; 概率密度函数的性质&nbsp;&nbsp;&nbsp;&nbsp; 均匀分布定义&nbsp; 均匀分布性质&nbsp; 指数分布定义&nbsp; 指数分布性质&nbsp;&nbsp;&nbsp; 指数分布用途&nbsp; 正态分布定义&nbsp;&nbsp; 用途&nbsp; 标准正态分布&nbsp; 标准正态分布表&nbsp; 标准正态分布性质&nbsp; 正态分布性质&nbsp; 3σ准则&nbsp;]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离散型随机变量]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E7%A6%BB%E6%95%A3%E5%9E%8B%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[离散型随机变量定义&nbsp;&nbsp;&nbsp; 0-1分布&nbsp; Bernoulli试验&nbsp;&nbsp; 二项分布&nbsp;&nbsp; 泊松分布&nbsp; 二项分布与泊松分布&nbsp;即当n&gt;10, p&lt;0.1时,二项分布B(n,p)可以用泊松分布π(np)来近似。&nbsp; 几何分布&nbsp;在重复多次的贝努里试验中，试验进行到某种结果出现第一次为止，此时的试验总次数服从几何分布。 如:射击，首次击中目标时射击的次数。]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论基础]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[频率与概率频率：频率是0~1之间的一个实数，在大量重复试验的基础上给出了随机事件发生可能性的估计。概率：当试验的次数增加时，随机事件A发生的频率的稳定值p称为概率。记为P(A)=p。 条件概率P(B|A)表示A发生的条件下, B发生的条件概率。P(B)!=P(B|A)，其原因在于事件A的发生改变了样本空间。&nbsp;&nbsp; 全概率公式与贝叶斯公式全概率公式&nbsp;&nbsp; 贝叶斯公式&nbsp; 事件独立性定义:设A，B是两随机事件，如果P(AB)=P(A)P(B)，则称A，B相互独立。两两独立并不能推出相互独立 随机变量&nbsp;随机变量分为离散型随机变量和连续性随机变量。]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率论与数理统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手写数字分类]]></title>
    <url>%2Fexamples%2F%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言MNIST问题可以称作深度学习中的“Hello World”。它与新闻分类一样，也是一个多分类问题。本例采用MNIST数据集，并且使用卷积神经网络模型来解决手写数字识别问题。 正文数据预处理本例采用训练集样本为60000，测试集样本为10000，每张图片大小均为28*28。对训练集、测试集样本转换为4D张量，并将其标准化。对训练集、测试集标签将其向量化。123456789(train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels) 构建网络采用一层卷积层，一层池化层，一层卷积层，一层池化层，一层卷积层，两层全连接层。在最后一层卷积层中，需要对3D数据转化为1D数据。123456789model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D(2, 2))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax')) 编译模型1234model.compile( optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 模型训练1model.fit(train_images, train_labels, epochs=5, batch_size=64) 结果与分析模型在测试集中进行评估，损失度与准确度为：0.027598469233673314 0.9916，比简单的密集连接模型的准确度97.8%要高。 源码123456789101112131415161718192021222324252627282930313233from keras import layersfrom keras import modelsfrom keras.datasets import mnistfrom keras.utils import to_categoricalimport numpy as np (train_images, train_labels), (test_images, test_labels) = mnist.load_data()train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.astype('float32') / 255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32') / 255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D(2, 2))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.compile( optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(train_images, train_labels, epochs=5, batch_size=64)test_loss, test_acc = model.evaluate(test_images, test_labels)print(test_loss, test_acc) 后记理解卷积神经网络的基本概念，即特征图、卷积、最大池化。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Multiclass Classification</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perceptron]]></title>
    <url>%2Fmachine%20learning%2FPerceptron%2F</url>
    <content type="text"><![CDATA[M-P神经元神经网络中最基本的成分就是神经元（neuron）模型。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个阈值（threshold），那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。M-P神经元就是将上述情形进行抽象所得，如下图。在上述模型中，神经元收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数（activation function”）处理以产生神经元的输出。这样把许多这样的神经元按一定的层次结构连接起来，就得到了神经网络。 感知机感知机（Perceptron）是神经网络和支持向量机的基础。 感知机原理感知机是二分类的线性模型，其输入是实例的特征向量，输出的是事例的类别，分别是+1和-1，属于判别模型。假设训练数据集是线性可分的，即存在一个线性超平面能将它们分开，而感知机的学习过程就一定会收敛（converge）而求得适当的权向量w=（w1;w2;…;wn);最后求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面，如下图。如果是非线性可分的数据，感知机学习过程将会发生振荡（fluctuation），w难以稳定下来，不能求得合适解，最后无法获得超平面。 感知机结构它有两层神经元组成，输入层接收外界输入信号传递给输出层，输出层是M-P神经元，如下图 感知机学习过程对于输出，权重wi(i=1,2,…,n)以及阈值θ可通过学习得到。阈值θ可看做一个固定输入为-1.0的“哑结点”所对应的连接权重w(n+1)，这样，权重和阈值的学习就可统一为权重的学习。感知机的学习规则非常简单，对训练样例(x,y)，若当前感知机的输出为，则感知机的权重调整为：其中称为学习率（learning rate）。 感知机解决非线性问题要解决非线性可分问题，需要考虑使用多层功能神经元。如下图则可用两层感知机解决问题。输出层与输入层之间的一层神经元，被称为隐含层（hidden layer），隐含层和输出层神经元都是拥有激活函数的功能神经元 多层前馈神经网络常见的神经网络如下图所示（由两层或两层以上的感知机所组成）。每一层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构通常称为“多层前馈神经网络”（multi-layer feedforward neural networks），其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值，神经网络“学”到的东西，蕴含在连接权与阈值之中。注：“前馈”是指网络拓扑结构上不存在环或回路 参考[1]感知机原理（perceptron）[2]周志华.机器学习.北京：清华大学出版社，2016年]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BackPropagation]]></title>
    <url>%2Fmachine%20learning%2FBackPropagation%2F</url>
    <content type="text"><![CDATA[BP算法简介对于非线性可分问题，需要考虑使用多层功能神经元，因此多层网络的学习能力比单层感知机强得多。而训练多层网络，感知机的学习规则就无法适用了。目前误差逆传播（error BackPropagation）算法是迄今最成功的的神经网络学习算法。BP算法不仅可用于多层前馈神经网络，还可用于其他类型的神经网络，如递归神经网络。 BP算法推导&nbsp;&nbsp;&nbsp; BP算法工作流程1、先将输入实例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；2、计算输出层的误差，再将误差逆向传播至隐层神经元，最后根据隐层神经元的误差来对连接权和阈值进行调整；3、1,2步骤迭代循环进行，直到达到某些停止条件为止，例如训练误差已达到一个很小的值。 累积BP算法与标准BP算法BP算法的目标是要最小化训练集D上的累积误差但上述推导的是“标准BP算法”：每次仅针对一个训练样例更新连接权和阈值；“累积BP算法”是基于累积误差最小化的算法。 标准BP算法每次更新只针对单个样例，参数更新得非常频繁，，而且对不同样例进行更新得效果可能会出现“抵消”现象。因此，为了达到同样的累积误差极小点，标准BP算法往往需要进行更多次的迭代。 累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后才对参数进行更新，其参数更新的频率低得多。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会更快获得较好的解。参考[1]周志华.机器学习.北京：清华大学出版社，2016年]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[预测房价]]></title>
    <url>%2Fexamples%2F%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%2F</url>
    <content type="text"><![CDATA[前言预测房价是一个回归问题，本例采用20世纪70年代中期波士顿房价数据集（boston_housing)，主要去预测房屋价格的中位数。并已知当时该郊区的一些数据点，如犯罪率、当地房产税率等。这些数据点（特征）都有不同的取值范围。该数据集样本数量较少，只有506个，分为404个训练样本和102个测试样本。 正文数据预处理因为各个特征的取值范围差异很大，如果直接输入，网络会自动适应取值不同的数据，学习效果会很差。所以，我们需要对每个特征进行标准化。标准化：（输入的每个特征-特征平均值）/ 标准差12345678# 数据预处理 标准化mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std 构建网络+编译本例网络采用两个隐藏层，一个输出层；隐藏层：采用Dense（全连接），隐藏单元均为64，激活函数为Relu；输出层：采用Dense，隐藏单元为1，无需激活函数，是一个线性层。这是标量回归（标量回归是预测单一连续值的回归）典型设置。 编译采用：优化器：rmsprop损失函数：MSE（mean squared error）衡量指标：MAE（mean absolute error）1234567def build_model(): model = models.Sequential() model.add(layers.Dense(64,activation='relu',input_shape=(train_data.shape[1],))) model.add(layers.Dense(64,activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) return model 模型训练因为本例的训练集数量过少，所以采用K折交叉验证来进行模型的评估。12345678910111213141516171819202122232425# K折交叉验证k = 4num_val_samples = len(train_data) // knum_epochs = 500all_mae_histories = []for i in range(k): print('processing fold #', i) val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[: i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[( i + 1 ) * num_val_samples:]], axis=0) model = build_model() history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1,verbose=0) mae_history = history.history['val_mean_absolute_error'] all_mae_histories.append(mae_history) 结果与分析计算所有轮次中的K折验证分数平均值12average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)] （以前从未发现还可以这样用TAT）绘制的验证分数如下：由该图发现，前十个点的取值范围与其他点不同，需删除；纵轴的范围过大，且数据方差也过大，难以看清图的规律，故我们采用指数移动平均值（EMA）来得到光滑的曲线。123456789def smooth_curve(points, factor=0.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points 123456smooth_mae_history = smooth_curve(average_mae_history[10:])plt.plot(range(1, len(smooth_mae_history) +1), smooth_mae_history)plt.xlabel('Epochs')plt.ylabel('Validation MAE')plt.show() 再次绘制的验证分数如下：由此发现，在第180轮后，不再显著降低（书上得出的结果和我的不同），之后开始过拟合。因此我们继续调整模型，epochs=180，并在测试集上进行测试，最后MAE为：2.7016119021995393 。在实际价格范围在10000~50000美元，预测房价与实际相差为2700美元左右。 源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from keras.datasets import boston_housingfrom keras import modelsfrom keras import layersimport numpy as npimport matplotlib.pyplot as plt(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()def build_model(): model = models.Sequential() model.add(layers.Dense(64,activation='relu',input_shape=(train_data.shape[1],))) model.add(layers.Dense(64,activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) return modeldef smooth_curve(points, factor=0.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points# 数据预处理 标准化mean = train_data.mean(axis=0)train_data -= meanstd = train_data.std(axis=0)train_data /= stdtest_data -= meantest_data /= std# K折交叉验证k = 4num_val_samples = len(train_data) // knum_epochs = 180all_mae_histories = []for i in range(k): print('processing fold #', i) val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[: i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[( i + 1 ) * num_val_samples:]], axis=0) model = build_model() history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1,verbose=0) mae_history = history.history['val_mean_absolute_error'] all_mae_histories.append(mae_history)"""average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]smooth_mae_history = smooth_curve(average_mae_history[10:])plt.plot(range(1, len(smooth_mae_history) +1), smooth_mae_history)plt.xlabel('Epochs')plt.ylabel('Validation MAE')plt.show()"""test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)print(test_mae_score) 后记 特征数据范围差异过大时，需要对特征进行标准化或归一化； 回归问题常用的损失函数是均方误差（MSE），常用的回归指标是平均绝对误差（MAE）； 数据过少时，可以采用K折交叉验证； 在绘制图形时，若遇到纵轴范围较大，可以采用指数移动平均值（EMA）来得到光滑的曲线。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Keras</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新闻分类]]></title>
    <url>%2Fexamples%2F%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言新闻分类是一个多分类问题，本例采用路透社数据集（reuters），该数据集将新闻分为互斥的46个不同的主题，分别有8982个训练样本和2246个测试样本。 正文数据预处理1、数据向量化。仍旧采用one-hot编码。博主发现python的赋值系统居然可以一个列表作为二维矩阵的索引值，直接全体进行赋值，以前从未在Java中感受过。2、标签向量化。因为标签值是一个标量，数据集的多个样本组成了向量。而to_categorical()则可以讲一个向量转化为由0/1组成的二维矩阵。123456789101112131415from keras.utils.np_utils import to_categoricaldef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)# 数据向量化，转化为2D张量x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)# 标签向量化y_train = to_categorical(train_labels)y_test = to_categorical(test_labels) 构建网络本例采用两层隐藏层，一层输出层；中间两层均采用Dense（全连接），隐藏单元均为64，激活函数均为Relu；第三层输出层采用Dense，隐藏单元为46（46个新闻分类），激活函数采用softmax。12345# 构建网络model = models.Sequential()model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))model.add(layers.Dense(64,activation='relu'))model.add(layers.Dense(46,activation='softmax')) 编译模型优化器：rmsprop损失函数：categorical_crossentropy（分类交叉熵）衡量指标：accuracy123model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 模型训练123456789101112# 验证集x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:]# 训练模型history = model.fit( partial_x_train, partial_y_train, epochs=40, batch_size=512, validation_data=(x_val, y_val)) 结果与分析训练与验证损失、训练与验证正确率曲线图如下所示：由此可分析，训练在第9轮后达到过拟合。为了降低过拟合，本例采用添加权重正则化、添加dropout正则化,训练轮数设为40。123456789# 构建网络model = models.Sequential()model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu',input_shape=(10000,)))model.add(layers.Dropout(0.5))model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu'))model.add(layers.Dropout(0.5))model.add(layers.Dense(46,activation='softmax')) 观察上图，发现过拟合问题得到缓解，在第20轮后出现过拟合，因此将训练轮数设为20。 测试集损失与准确率1results = model.evaluate(x_test, y_test) 损失与准确率：[1.2500886322660099 0.7640249332146037] 预测12predictions = model.predict(x_test)print(np.argmax(predictions[0])) 源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from keras import modelsfrom keras import layersfrom keras import regularizersfrom keras.datasets import reutersfrom keras.utils.np_utils import to_categoricalimport numpy as npimport matplotlib.pyplot as pltdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)# 数据向量化，转化为2D张量x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)# 标签向量化y_train = to_categorical(train_labels)y_test = to_categorical(test_labels)# 构建网络model = models.Sequential()model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu',input_shape=(10000,)))model.add(layers.Dropout(0.5))model.add(layers.Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu'))model.add(layers.Dropout(0.5))model.add(layers.Dense(46,activation='softmax'))# 编译模型model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])# 验证集x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = y_train[:1000]partial_y_train = y_train[1000:]# 训练模型history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"""# 绘制损失loss = history.history['loss']val_loss = history.history['val_loss']epochs = range(1, len(loss)+1)plt.plot(epochs, loss, 'bo', label='Training loss')plt.plot(epochs, val_loss, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show()# 绘制精确acc = history.history['acc']val_acc = history.history['val_acc']plt.clf()plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show()""""""results = model.evaluate(x_test, y_test)print(results)"""predictions = model.predict(x_test)print(np.argmax(predictions[0])) 后记降低过拟合是整个深度学习的核心问题。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Multiclass Classification</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电影评论分类]]></title>
    <url>%2Fexamples%2F%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言电影评论分类是一个二分类问题，为了将电影评论区分为正面评论和负面评论。本例采用IMDB数据集，它包含互联网电影数据库（IMDB）的50000条严重两极化评论。数据集被分为用于训练的25000条评论与用于测试的25000条评论。IMDB数据集已经经过预处理：评论（单词序列）已被转换为整数序列，其中每个整数代表字典中的某个单词（按使用频率进行排列）。 正文数据加载本例保留训练数据中前5000个最常出现的单词，低频单词将被舍弃。(因电脑原因，改为5000)12from keras.datasets import imdb(train_data, train_lables), (test_data, test_labels) = imdb.load_data(num_words=5000) 注：实验时，发现数据下载不下来，只能通过本地下载，再将下载后的数据放在 C:\Users\Administrator.keras\datasets文件夹下 数据预处理需要将样本数据的整数序列转换为2D张量。本例采用one-hot编码，将其转换为01的向量。Dense层（全连接层）可以处理浮点数向量数据。12345678import numpy as np def vectorize_sequences(sequences, dimension=5000): results = np.zeros((len(sequences),dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return resultsx_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data) 将标签向量化12y_train = np.asarray(train_lables).astype('float32')y_test = np.asarray(test_lables).astype('float32') 构建网络本例采用：两个中间层（均为全连接层），隐藏单元（hidden unit）个数为16，激活函数为relu（rectified linear unit）；第三层输出一个标量，预测当前情感，激活函数为sigmoid函数；1234567from keras import modelsfrom keras import layersmodel = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(5000,)))model.add(layers.Dense(16,activation='relu'))model.add(layers.Dense(1,activation='sigmoid')) 编译模型本例采用：损失函数：binary_crossentropy(二元交叉熵损失)优化器：rmsprop优化器1234model.compile( optimizer='rmsprop', loss='bianry_crossentropy', metrics=['accuracy']) 训练模型123456789101112x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:]history = model.fit( partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val,y_val)) 结果分析绘制训练损失和验证损失123456789101112131415import matplotlib.pyplot as plthistory_dict = history.historyloss_values = history_dict['loss']val_loss_values = history_dict['val_loss']epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, 'bo', label='Training loss')plt.plot(epochs, val_loss_values, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show() 绘制训练精度和验证精度12345678910plt.clf()acc = history_dict['acc']val_acc = history_dict['val_acc']plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.title('Training and validation accuracy')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show() 由两图可知，训练损失不断降低，训练精度不断升高。而验证损失却在第4轮达到最低后又继续上升，验证精度在第4轮达到最高后又继续降低。这个原因是因为过拟合，对数据过度优化，导致学到的表示仅适用于训练数据，无法泛化到训练集之外的数据。因此需要在第四轮之后停止训练。123456history = model.fit( partial_x_train, partial_y_train, epochs=4, batch_size=512, validation_data=(x_val,y_val)) 测试集损失与准确率训练轮数在四轮结束后，测试集的损失和准确率结果如下：[0.29896098415374756, 0.87688] 源代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from keras.datasets import imdbfrom keras import modelsfrom keras import layersimport numpy as npimport matplotlib.pyplot as plt# one-hot编码def vectorize_sequences(sequences, dimension=5000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results(train_data, train_labels), (test_data, test_labels) = imdb.load_data( num_words=5000)# 样本数据转换为2D张量x_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data)# 标签向量化y_train = np.asarray(train_labels).astype('float32')y_test = np.asarray(test_labels).astype('float32')x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:]# 模型构建model = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(5000,)))model.add(layers.Dense(16,activation='relu'))model.add(layers.Dense(1,activation='sigmoid'))# 编译model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])# 训练history = model.fit(partial_x_train, partial_y_train,epochs=4,batch_size=512,validation_data=(x_val,y_val))history_dict = history.historyloss_values = history_dict['loss']val_loss_values = history_dict['val_loss']epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, 'bo', label='Training loss')plt.plot(epochs, val_loss_values, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show()plt.clf()acc = history_dict['acc']val_acc = history_dict['val_acc']plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.title('Training and validation accuracy')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show()## 预测results = model.evaluate(x_test, y_test)print(results) 后记可以通过降低过拟合来进一步优化，提高预测精度]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Keras</tag>
        <tag>Binary Classification</tag>
      </tags>
  </entry>
</search>
